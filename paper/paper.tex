%\documentclass[aps,twocolumn,longbibliography,english,superscriptaddress]{revtex4-1}
\documentclass{article}
\usepackage{iclr2021_conference}
%\documentclass[a4paper,superscriptaddress,11pt]{article}
\pdfoutput=1
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{quoting}
\usepackage{upquote}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage[framemethod=TikZ]{mdframed}
\usetikzlibrary{shapes}
\usepackage{wrapfig}
%\usepackage{caption}
%\usepackage[plain]{algorithm}
\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{rotating}
%\usepackage{cite}
\usepackage{booktabs}
%\usepackage{unicode-math}
%\usepackage{algorithm}% http://ctan.org/pkg/algorithm
%\usepackage{algpseudocode}% http://ctan.org/pkg/algpseudocode
\usepackage{xcolor}% http://ctan.org/pkg/xcolor
\makeatletter
\newsavebox{\@brx}
\newcommand{\llangle}[1][]{\savebox{\@brx}{\(\m@th{#1\langle}\)}%
  \mathopen{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\newcommand{\rrangle}[1][]{\savebox{\@brx}{\(\m@th{#1\rangle}\)}%
  \mathclose{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\makeatother
\usepackage{bbm}
\usepackage{jlcode}
\usepackage{graphicx}
\usepackage{amsmath,color,amsthm}
\usepackage{mathrsfs}
\usepackage{float}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{indentfirst}
\usepackage{txfonts}
\usepackage[epsilon, tsrm, altpo]{backnaur}

\DeclareMathAlphabet{\mymathbb}{U}{BOONDOX-ds}{m}{n}
\newcommand{\listingcaption}[1]%
{%
\refstepcounter{lstlisting}\hfill%
Listing \thelstlisting: #1\hfill%\hfill%
}%
\newcolumntype{b}{X}
\newcolumntype{s}{>{\hsize=.7\hsize}X}
\usepackage{listings}
\lstset{
    language=Julia,
    basicstyle=\ttfamily\scriptsize,
    numberstyle=\scriptsize,
    % numbers=left,
    backgroundcolor=\color{gray!7},
    %backgroundcolor=\color{white},
    %frame=single,
    xleftmargin=2em,
    tabsize=2,
    rulecolor=\color{black!15},
    %title=\lstname,
    escapeinside={(*}{*)},
    breaklines=true,
    %breakatwhitespace=true,
    %framextopmargin=2pt,
    %framexbottommargin=2pt,
    frame=bt,
    extendedchars=true,
    inputencoding=utf8,
    columns=fullflexible,
    %escapeinside={(*@}{@*)},
}

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=1000
\hbadness=1000

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

%Journal reference.  Comma sets off: name, vol, page, year
\def\journal #1, #2, #3, 1#4#5#6{{\sl #1~}{\bf #2}, #3 (1#4#5#6) }
\def\pr{\journal Phys. Rev., }
\def\prb{\journal Phys. Rev. B, }
\def\prl{\journal Phys. Rev. Lett., }
\def\pl{\journal Phys. Lett., }
%\def\np{\journal Nucl. Phys., }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\usepackage{CJK}
%\usepackage[colorlinks, citecolor=blue]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}

%%%%%% Shortcut related
\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}
\newcommand{\out}{{\vx^L}}
\newcommand{\inp}{{\vx^0}}
\newcommand{\cquad}{{{ }_{\quad}}}
\newcommand{\pluseq}{\mathrel{+}=}
\newcommand{\minuseq}{\mathrel{-}=}
\newcommand{\vx}{{\mathbf{x}}}
\newcommand{\vg}{{\mathbf{g}}}
\newcommand{\vp}{{\mathbf{p}}}
\newcommand{\vy}{{\mathbf{y}}}
\newcommand{\Var}{{\mathrm{Var}}}
\newcommand{\Mean}{{\mathrm{E}}}
\newcommand{\vvalue}{{\texttt{value}}}
\newcommand{\grad}{{\texttt{grad}}}
\newcommand{\parameter}{{\texttt{parameter}}}
%%%%%% Convention related
\newcommand{\SWAP}{{\rm SWAP}}
\newcommand{\CNOT}{{\rm CNOT}}
\newcommand{\bigO}{{\mathcal{O}}}
\newcommand{\X}{{\rm X}}
\renewcommand{\H}{{\rm H}}
\newcommand{\Rx}{{\rm Rx}}
\renewcommand{\v}[1]{{\bf #1}}
\newcommand{\dataset}{{\mathcal{D}}}
\newcommand{\wfunc}{{\psi}}
\newcommand{\SU}{{\rm SU}}
\newcommand{\UU}{{\rm U}}
\newcommand{\thetav}{{\boldsymbol{\theta}}}
\newcommand{\gammav}{{\boldsymbol{\gamma}}}
\newcommand{\thetai}{{\theta^\alpha_l}}
\newcommand{\Expect}{{\mathbb{E}}}
\newcommand{\Tr}{{\rm Tr}}
\renewcommand{\cite}[1]{{\citep{#1}}}
\newcommand{\etc}{{\it etc~}}
\newcommand{\etal}{{\it etal~}}
\newcommand{\xset}{\mathbf{X}}
\newcommand{\fl}{\texttt{fl}}
\newcommand{\pdata}{\mathbf{\pi}}
\newcommand{\q}{\mathbf{q}}
\newcommand{\epdata}{\mathbf{\hat{\pi}}}
\newcommand{\gammaset}{\boldsymbol{\Gamma}}
\newcommand{\ei}{{\mathbf{e}_l^\alpha}}
\newcommand{\vtheta}{{\boldsymbol{\theta}}}
\newcommand{\sigmag}{{\nu}}
\newcommand{\sigmai}[2]{{\sigma^{#2}_{#1}}}
\newcommand{\qi}[1]{{q^{\alpha_{#1}}_{#1}}}
\newcommand{\BAS}{Bars-and-Stripes}
\newcommand{\circled}[1]{\raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {#1}}}}
\newcommand{\qexpect}[1]{{\left\langle #1\right\rangle}}
\newcommand{\expect}[2]{{\mathop{\mathbb{E}}\limits_{\substack{#2}}\left[#1\right]}}
\newcommand{\var}[2]{{\mathop{\mathrm{Var}}\limits_{\substack{#2}}\left(#1\right)}}
\newcommand{\pshift}[1]{{p_{\thetav+#1}}}
\newcommand{\upcite}[1]{\textsuperscript{\cite{#1}}}
\newcommand{\Eq}[1]{Eq.~(\ref{#1})}
\newcommand{\Fig}[1]{Fig.~\ref{#1}}
\newcommand{\Lst}[1]{Listing.~\ref{#1}}
\newcommand{\Tbl}[1]{Table~\ref{#1}}
\newcommand{\Sec}[1]{Sec.~\ref{#1}}
\newcommand{\App}[1]{Appendix~\ref{#1}}
\newcommand{\bra}[1]{\mbox{$\left\langle #1 \right|$}}
\newcommand{\ket}[1]{\mbox{$\left| #1 \right\rangle$}}
\newcommand{\braket}[2]{\mbox{$\left\langle #1 | #2 \right\rangle$}}
\newcommand{\tr}[1]{\mathrm{tr}\mbox{$\left[ #1\right]$}}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

%%%%%% Comment related
\newcommand{\red}[1]{[{\bf  \color{red}{LW: #1}}]}
\newcommand{\xred}[1]{[{\bf  \color{red}{\sout{LW: #1}}}]}
\newcommand{\blue}[1]{[{\bf  \color{blue}{JG: #1}}]}
\newcommand{\violet}[1]{[{\bf  \color{violet}{MLS: #1}}]}
\newcommand{\green}[1]{[{\bf  \color{green}{TZ: #1}}]}
\newcommand{\xgreen}[1]{[{\bf  \color{green}{\sout{TZ: #1}}}]}
\newcommand{\xblue}[1]{[{\bf  \color{blue}{\sout{JG: #1}}}]}
\newcommand{\material}[1]{\iffalse[{\bf  \color{cyan}{Material: #1}}]\fi}
\newcommand{\orange}[1]{\iffalse[{\bf  \color{orange}{Jo: #1}}]\fi}

\newtheorem{theorem}{\textit{Theorem}}
\newtheorem{corollary}{\textit Branching Rule}
\theoremstyle{definition}\newtheorem{definition}{\textit{Definition}}

\makeatother

\iclrfinalcopy
\begin{document}
\title{Solving the maximum independant set problem by generic programming einsum networks}

\author{Jin-Guo Liu\\
Harvard University\\
\texttt{jinguoliu@g.harvard.edu}\\
\AND
Xun Gao\\
Harvard University\\
\texttt{xungao@g.harvard.edu}\\
}
\maketitle

\begin{abstract}
	Solving the maximum independent set size problem by mapping the graph to an einsum network. 
    By contracting the einsum network with generic element types, we show how to obtain the maximum independent set size,
    the independence polynomial and optimal configurations.
\end{abstract}

\section{Introduction}
Branching algorithms can solve the MIS problem in $1.1893^n n ^{O(1)}$ time ~\cite{Xiao2017}.
Previous dynamic programming approach~\cite{Fomin2013} can reduce the complexity of computing to $2^{tw(G)}$.

\section{A short introduction to einsum networks}

The word ``einsum'' is a shorthand for Einstein's summation, however, modern einsum notation in program is actually invented by a group of programmers.
Einstein's notation is originally proposed as a generalization to of multiplication between two matrices to the contraction between multiple tensors.
Let $A, B$ be two matrices, the matrix multiplication is defined as $C_{ik} = \sum_{j}A_{ij}B_{jk}$.
It is denoted as $C_i^k = A_i^j B_j^k$ in the Einstein's original notation, where the paired subscript and superscript $j$ is a dummy index summed over.
An example of tensor networks is $C_i^l = A_{ij}^k B^l_k V^j$.
One can map a tensor network to a muti-graph with open edges by viewing a tensor in the expression on the right hand side as a vertex in a graph, a label pairing two tensors as an edge, and the remaining labels as open edges.
We get the graphical notation as the following.

\centerline{\begin{tikzpicture}
    \def\dx{0};
    \def\r{0.25}
    \def\ax{0}
    \def\ay{0}
    \def\bx{1}
    \def\by{1}
    \def\cx{1}
    \def\cy{-1}
    \filldraw[fill=black] (\ax+\dx,\ax) circle [radius=\r];
    \filldraw[fill=black] (\bx+\dx,\by) circle [radius=\r];
    \filldraw[fill=black] (\cx+\dx,\cy) circle [radius=\r];
    %\draw [black,thick] (\ax+\dx,\ay) .. controls (0.5*\ax+0.5*\bx+\dx+0.2,0.5*\ay+0.5*\by-0.2) .. (\bx+\dx,\by);
    %\draw [black,thick] (\ax+\dx,\ay) .. controls (0.5*\ax+0.5*\bx+\dx-0.2,0.5*\ay+0.5*\by+0.2) .. (\bx+\dx,\by);
    \draw [black,thick] (\ax+\dx,\ay) -- (\bx+\dx,\by);
    \draw [black,thick] (\ax+\dx,\ay) -- (\cx+\dx,\cy);
    \draw [black,thick] (\ax+\dx,\ay) -- (\ax+\dx-0.8,\ay);
    \draw [black,thick] (\bx+\dx,\by) -- (\bx+\dx+0.8,\by);
    \node[color=white] at (\ax+\dx,\ax) {A};
    \node[color=white] at (\bx+\dx,\by) {B};
    \node[color=black] at (0.5*\ax+0.5*\bx-0.2+\dx,0.5*\ay+0.5\by+0.2) {k};
    \node[color=black] at (0.5*\ax+0.5*\cx+\dx+0.2,0.5*\ay+0.5\cy+0.2) {j};
    \node[color=black] at (\ax+\dx-0.5,\ay-0.2) {i};
    \node[color=black] at (\bx+\dx+0.5,\by-0.2) {l};
    \node[color=white] at (\cx+\dx,\cy) {V};
\end{tikzpicture}}

One can easily check a label in a tensor network representation appears precisely twice.
Numpy programmers make a generalization to this notation by not restricting the number of times a label is used by tensors.
For example, $C_{ijk} = A_{jkm} B_{mil} V_{jm}$ is an einsum but not a tensor network. Here, all indices not appearing in the output are summed over, i.e. it represents $C_{ijk} = \sum_{ml}A_{jkm} B_{mia} V_{jm}$.
Whether the index appear as a superscript or a subscript makes no sense now.
The graphical representation of an einsum is a hypergraph, where an edge can be shared by a arbituary number of nodes.

\centerline{\begin{tikzpicture}
    \def\dx{0};
    \def\r{0.25}
    \def\ax{0}
    \def\ay{0}
    \def\bx{1}
    \def\by{1}
    \def\cx{1}
    \def\cy{-1}
    \filldraw[fill=black] (\ax+\dx,\ax) circle [radius=\r];
    \filldraw[fill=black] (\bx+\dx,\by) circle [radius=\r];
    \filldraw[fill=black] (\cx+\dx,\cy) circle [radius=\r];
    \draw[color=blue,thick] (\bx+\dx,\by) circle [radius=\r+0.1];
    \node[color=blue] at (\bx+\dx-0.6,\bx) {i};
    \draw[color=cyan,dashed,thick] (\bx+\dx,\by) circle [radius=\r+0.25];
    \node[color=cyan] at (\bx+\dx+0.5,\bx-0.5) {l};
    \draw[color=violet,thick,dashed] (\ax+\dx,\ay) circle [radius=\r+0.1];
    \node[color=violet] at (\ax+\dx+0.4,-0.4) {k};
    \draw[color=black,thick] (0.5+\dx,0) circle [radius=1.7];
    \node[color=black] at (0.5+\dx-1.9,0) {m};
    \draw[color=red,thick,rotate=135,dashed] (-0.6,0) ellipse (1.2 and 0.5);
    \node[color=red] at (\ax+\dx,-1.0) {j};
    \node[color=white] at (\ax+\dx,\ax) {A};
    \node[color=white] at (\bx+\dx,\by) {B};
    \node[color=white] at (\cx+\dx,\cy) {V};
\end{tikzpicture}}

In the main text, we stick to the einsum notation rather than the tensor network notation,
although one can easily translate an einsum network to the equivalent tensor network by adding $\delta$ tensors (a generalization of identity matrix to higher order).
We do not use the language of tensor network because it can sometime increase the contraction complexity of a graph. We will show an example in the appendix.

\section{Independence polynomial}
\begin{figure*}[h!]
    \centerline{\begin{tikzpicture}
        \def\dx{0};
        \def\r{0.25}
        \filldraw[fill=black] (\dx,0) circle [radius=\r];
        \filldraw[fill=black] (\dx,1.5) circle [radius=\r];
        \filldraw[fill=black] (1.5+\dx,0) circle [radius=\r];
        \filldraw[fill=black] (1.5+\dx,1.5) circle [radius=\r];
        \filldraw[fill=black] (2.5+\dx,2.5) circle [radius=\r];
        \draw [black,thick] (\dx,0) -- (\dx,1.5);
        \draw [black,thick] (\dx,0) -- (1.5+\dx,0);
        \draw [black,thick] (\dx,1.5) -- (1.5+\dx,1.5);
        \draw [black,thick] (1.5+\dx,0) -- (1.5+\dx,1.5);
        \draw [black,thick] (1.5+\dx,0) -- (\dx,1.5);
        \draw [black,thick] (2.5+\dx,2.5) -- (1.5+\dx,1.5);
        \node[color=white] at (\dx,0) {a};
        \node[color=white] at (\dx,1.5) {b};
        \node[color=white] at (1.5+\dx,0) {c};
        \node[color=white] at (1.5+\dx,1.5) {d};
        \node[color=white] at (2.5+\dx,2.5) {e};
        \def\dx{5};
        \def\r{0.25}
        \draw [black,thick] (\dx,0) -- (\dx,1.5);
        \draw [black,thick] (\dx,0) -- (1.5+\dx,0);
        \draw [black,thick] (\dx,1.5) -- (1.5+\dx,1.5);
        \draw [black,thick] (1.5+\dx,0) -- (1.5+\dx,1.5);
        \draw [black,thick] (1.5+\dx,0) -- (\dx,1.5);
        \draw [black,thick] (2.5+\dx,2.5) -- (1.5+\dx,1.5);
        \node[color=black,fill=white] at (0.75+\dx,0) {\scriptsize $B$};
        \node[color=black,fill=white] at (\dx,0.75) {\scriptsize $B$};
        \node[color=black,fill=white] at (1.5+\dx,0.75) {\scriptsize $B$};
        \node[color=black,fill=white] at (0.75+\dx,1.5) {\scriptsize $B$};
        \node[color=black,fill=white] at (0.75+\dx,0.75) {\scriptsize $B$};
        \node[color=black,fill=white] at (2+\dx,2) {\scriptsize $B$};
        \node[color=black,fill=white] at (\dx,0) {\scriptsize $W(x_a)$};
        \node[color=black,fill=white] at (\dx,1.5) {\scriptsize $W(x_b)$};
        \node[color=black,fill=white] at (1.5+\dx,0) {\scriptsize $W(x_c)$};
        \node[color=black,fill=white] at (1.5+\dx,1.5) {\scriptsize $W(x_d)$};
        \node[color=black,fill=white] at (2.5+\dx,2.5) {\scriptsize $W(x_e)$};
    \end{tikzpicture}}
    \caption{Mapping a graph to an einsum network.}\label{fig:einsummapping}
\end{figure*}

Let us map the graph $G$ into an einsum network, as shown in \Fig{fig:einsummapping}, by placing a rank one tensor of size $2$ on vertex $i$
\begin{equation}
    W(x_i)_{s_i} = \left(\begin{matrix}
        1 \\
        x_i
    \end{matrix}\right)_{s_i},
\end{equation}
and a rank two tensor of size $2 \times 2$ on edge $(i,j)$
\begin{equation}
    B_{s_i s_j} = \left(\begin{matrix}
        1  & 1\\
        1 & 0
    \end{matrix}\right)_{s_is_j},
\end{equation}
where a tensor index $s_i$ is a boolean variable that being 1 if vertex $i$ is in the independent set, 0 otherwise, $x_i$ is a variable.
We denote the contraction result of this einsum network as
\begin{equation}
    A(G, \{x_1,\ldots,x_n\}) = \sum\limits_{s_1, s_2, \ldots, s_n = 0}^{1} \prod\limits_{i=1}^n W(x_i)_{s_i} \prod\limits_{(i,j) \in E(G)} B_{s_i s_j}.
\end{equation}
Here, the einsum runs over all possible vertex configurations and accumulates the product of tensor elements to the output.
Let $x_i = x$ be the same variable, then the product over vertex tensors provides a factor $x^k$, where $k=\sum_i s_i$ is the vertex set size, 
and the product over edge tensors provides a factor $0$ for configurations not being an independent set.
The contraction of this einsum network gives the independence polynomial~\cite{Ferrin2014, Harvey2017} of $G$
\begin{equation}
I(G, x) = \sum_{k=1}^{\alpha(G)} a_k x^k,
\end{equation}
where $a_k$ is the number of independent sets of size $k$ in $G$, and $\alpha(G)$ is the maximum independent set size.
By mapping the independence polynomial solving problem to the einsum network contraction, one can take the advantage of recently developed techiniques in tensor network based quantum circuit simulations ~\cite{Gray2021,Pan2021},
where people evaluate a tensor network by pairwise contracting tensors in a heuristic order.
A good contraction order can reduce the time complexity significantly, at the cost of having a space overhead of $O(2^{tw(G)})$, where $tw(G)$ is the treewidth of $G$.~\cite{Markov2008}
The pairwise tensor contraction also makes it possible to utilize fast basic linear algebra subprograms (BLAS) functions for certain tensor element types.

Before contracting the einsum network and evaluating the independence polynomial numerically, let us first give up thinking $0$s and $1$s in tensors $W(x)$ and $B$ as regular computer numbers such as integers and floating point numbers.
Instead, we treat them as the additive identity and multiplicative identity of a commutative semiring.
A semiring is a ring without additive inverse, while a commutative semiring is a semiring that multiplication commutative.
To define a commutative semiring with addition algebra $\oplus$ and multiplication algebra $\odot$ on a set $R$, the following relation must hold for arbituary three elements $a, b, c \in R$.
\begin{align*}
(a \oplus b) \oplus c = a \oplus (b \oplus c) & \hspace{5em}\text{$\triangleright$ commutative monoid $\oplus$ with identity $\mymathbb{0}$}\\
a \oplus \mymathbb{0} = \mymathbb{0} \oplus a = a &\\
a \oplus b = b \oplus a &\\
&\\
(a \odot b) \odot c = a \odot (b \odot c)  &   \hspace{5em}\text{$\triangleright$ commutative monoid $\odot$ with identity $\mymathbb{1}$}\\
a \odot  \mymathbb{1} =  \mymathbb{1} \odot a = a &\\
a \odot b = b \odot a &\\
&\\
a \odot (b\oplus c) = a\odot b + a\odot c  &  \hspace{5em}\text{$\triangleright$ left and right distributive}\\
(a\oplus b) \odot c = a\odot c \oplus b\odot c &\\
&\\
a \odot \mymathbb{0} = \mymathbb{0} \odot a = \mymathbb{0}
\end{align*}

In the rest of this paper, we show how to obtain the independence polynomial, the maximum independent set size and optimal configurations of a general graph $G$ by designing tensor element types as commutative semirings,
i.e. making the einsum network programming generic~\cite{Stepanov2014}.

\subsection{The polynomial approach}
A straight forward approach to evaluate the independence polynomial is treating the tensor elements as polynomials, and evaluate the polynomial directly.
Let us create a polynomial type, and represent a polynomial $a_0 + a_1 x + \ldots + a_k x^k$ as a vector $(a_0, a_1, \ldots, a_k) \in R^k$, e.g. $x$ is represented as $(0, 1)$.
We define the algebra between the polynomials $a$ of order $k_a$ and $b$ of order $k_b$ as
\begin{align}
    \begin{split}
    a \oplus b &= (a_0 + b_0, a_1 + b_1, \ldots, a_{\max(k_a, k_b)} + b_{\max(k_a, k_b)}),\\
    a \odot b &= (a_0 + b_0, a_1b_0 + a_0b_1, \ldots, a_{k_a} b_{k_b}),\\
    \mymathbb{0} &= (),\\
    \mymathbb{1} &= (1).
    \end{split}
\end{align}
By contracting the einsum network with polynomial type, the final result is the exact representation of the independence polynomial.
In the program, the multiplication can be evaluated efficiently with the convolution theorem.
The only problem of this method is it suffers from a space overhead that propotional to the maximum independant set size because each polynomial requires a vector of such size to store the factors.
In the following subsections, we managed to solve this problem.

\subsection{The fitting and Fourier transformation approaches}
Let $m=\alpha(G)$ be the maximum independent set size and $X$ be a set of $m+1$ random real numbers, e.g. $\{0, 1, 2, \ldots, m\}$.
We compute the einsum contraction for each $x_i \in X$ and obtain the following relations
\begin{align}
    \begin{split}
a_0 + a_1 x_1 + a_1 x_1^2 + \ldots + a_m x_1^m &= y_0\\
a_0 + a_1 x_2 + a_2 x_2^2 + \ldots + a_m x_2^m &= y_1\\
\ldots&\\
a_0 + a_1 x_m + a_2 x_m^2 + \ldots + a_m x_m^m& = y_m
    \end{split}
\end{align}
The polynomial fitting between $X$ and $Y = \{y_0, y_1, \ldots, y_m\}$ gives us the factors.
The polynomial fitting is esentially about solving the following linear equation
\begin{align}
\left(\begin{matrix}
1 & x_1 & x_1^2 & \ldots & x_1^m \\
1 & x_2 & x_2^2 & \ldots & x_2^m \\
\vdots & \vdots & \vdots &\ddots & \vdots \\
1 & x_m & x_m^2 & \ldots & x_m^m
\end{matrix}\right)
\left(\begin{matrix}
a_0 \\ a_1 \\ \vdots \\ a_m
\end{matrix}\right)
= \left(\begin{matrix}
y_0 \\ y_1 \\ \vdots \\ y_m
\end{matrix}\right).
\end{align}

In practise, the fitting can suffer from the non-negligible round off errors of floating point operations and produce unreliable results.
This is because the factors of independence polynomial can be different in magnitude by many orders.
Instead of choosing $X$ as a set of random real numbers, we make it form a geometric sequence in the complex domain $x_j = r\omega^j$, where $r \in \mathbb{R}$ and $\omega = e^{-2\pi i/(m+1)}$. The above linear equation becomes
\begin{align}
\left(\begin{matrix}
1 & r\omega & r^2\omega^2 & \ldots & r^m\omega^m \\
1 & r\omega^2 & r^2\omega^4 & \ldots & r^m\omega^{2m} \\
\vdots & \vdots & \vdots &\ddots & \vdots \\
1 & r\omega^m & r^2\omega^{2m} & \ldots & r^m\omega^{m^2}
\end{matrix}\right)
\left(\begin{matrix}
a_0 \\ a_1 \\ \vdots \\ a_m
\end{matrix}\right)
= \left(\begin{matrix}
y_0 \\ y_1 \\ \vdots \\ y_m
\end{matrix}\right).
\end{align}

Let us rearange the factors $r^j$ to $a_j$, the matrix on left side is exactly the a descrete fourier transformation (DFT) matrix.
Then we can obtain the factors using the inverse fourier transformation $\vec a_r = {\rm FFT^{-1}}(\omega) \cdot \vec y$, where $(\vec a_r)_j = a_j r ^j$.
By choosing diferent $r$, one can obtain better precision in low independant set size region  ($\omega<1$) and high independant set size region ($\omega>1$).

\subsection{The finite field algebra approach}
It is possible to compute the independence polynomials exactly using 64 bit integers types only,
even when the factors are larger than that can be represented by 64 bit integers.
We achieve this by designing a finite field algebra $GF(p)$

\begin{align}
\begin{split}
    x ~\oplus~ y &= x+y\pmod p,\\
    x ~\odot~ y &= xy\pmod p,\\
    \mymathbb{0} &= 0,\\
    \mymathbb{1} &= 1.
\end{split}
\end{align}

In a finite field algebra, we have the following observations
\begin{enumerate}
    \item One can still use Gaussian elimination~\cite{Golub2013} to solve a linear equation.
    This is because a field has the property that the multiplicative inverse exists for any non-zero value.
    The multiplicative inverse here can be computed with the extended Euclidean algorithm.
    \item Given the remainders of a larger integer $x$ over a set of coprime integers $\{p_1, p_2, \ldots, p_n\}$,
    $x \pmod {p_1 \times p_2 \times \ldots \times p_n}$ can be computed using the chinese remainder theorem.
    With this, one can infer big integers even though its bit width is larger than the register size.
\end{enumerate}
With these observations, we developed Algorithm~\ref{alg:finitefield} to compute independent polynomial exactly without introducing space overheads.
In the algorithm, except the computation of chinese remainder theorem, all computations are done with integers with fixed width $W$.

\begin{algorithm}[!ht]
    \small
    \SetAlgoNoLine
    \LinesNumbered
    Let $P = 1$, vector $X = (0,1,2,\ldots,m)$, matrix $\hat X_{ij} = X_i^j$, where $i,j = 0, 1, \ldots m$\;
    \While{true}{
        compute the largest prime $p$ that $\gcd(p, P) = 1 \land p \leq 2^W$\;
        compute the tensor network contraction on $GF(p)$ and obtain $Y = (y_0, y_1, \ldots , y_m) \pmod p $\;
        $A_p = (a_0, a_1, \ldots, a_m) \pmod p = {\rm gaussian\_elimination}(\hat X, Y \pmod p) $\;
        $A_{P\times p} = {\rm chinese\_remainder}(A_P, A_p)$\;
        \If{$A_P = A_{P \times p}$}{
            \Return $A_P$ \tcp*[l]{converged}
        }
        $P = P \times p$\;
    }\caption{Compute independence polynomial exactly without integer overflow}\label{alg:finitefield}
\end{algorithm}

\section{Computing maximum independent set size and its corresponding degeneracy and configurations}
Obtaining the maximum independent set size and its degeneracy can be computational more efficient. Let $x=\infty$, then the independence polynomial becomes
\begin{equation}
I(G, \infty) = a_k \infty^{\alpha(G)},
\end{equation}
where the lower orders terms disappear automatically. We can define a new algebra as
\begin{align}
\begin{split}
    a_x\infty^x \oplus a_y\infty^y &= \begin{cases}
        (a_x + a_y)\infty^{\max(x,y)}, & x = y\\
        a_y\infty^{\max(x,y)}, & x < y\\
        a_x\infty^{\max(x,y)}, & x > y
    \end{cases}\\
    a_x\infty^x \odot a_y\infty^y &= a_x a_y\infty^{x+y}\\
    \mymathbb{0} &= 0\infty^{-\infty}\\
    \mymathbb{1} &= 1\infty^{0}
\end{split}
\end{align}
In the program, we only store the power $x$ and the corresponding factor $a_x$ that initialized to $1$.
This algebra is consistent with the one we derived in ~\cite{Liu2021} that uses the tropical tensor network for solving spin glass ground states.
If one is only interested in obtaining $\alpha(G)$, he can drop the factor parts, then the algebra of $x$ becomes the max-plus tropical algebra~\cite{Maclagan2015,Moore2011}.

One may also want to obtain all ground state configurations, it can be achieved replacing the factors $a_x$ with a set of bit strings $s_x$.
We design a new element type that having algebra
\iffalse
\begin{align}
\begin{split}
    s_x \oplus s_y &= s_x \cup s_y\\
    s_x \odot s_y &= \{\sigma \lor \tau | \sigma \in s_x, \tau \in s_y\}\\
    \mymathbb{0} &= \{\}\\
    \mymathbb{1} &= \{\boldsymbol 0\}
\end{split}
\end{align}
\fi
\begin{align}
\begin{split}
    s_x\infty^x \oplus s_y\infty^y &= \begin{cases}
        (s_x \cup s_y)\infty^{\max(x,y)}, & x = y\\
        s_y\infty^{\max(x,y)}, & x < y\\
        s_x\infty^{\max(x,y)}, & x > y
    \end{cases},\\
    s_x\infty^x \odot s_y\infty^y &= \{\sigma + \tau | \sigma \in s_x, \tau \in s_y\}\infty^{x+y},\\
    \mymathbb{0} &= \{\}\infty^{-\infty},\\
    \mymathbb{1} &= \{\boldsymbol 0\}\infty^{0},
\end{split}
\end{align}
One can easily check that this replacement does not change the fact that the algebra is a commutative semiring.
We first initialize the bit strings of the variable $x$ in the vertex tensor to a vertex index $i$ dependent onehot vector $x_i = \boldsymbol{e}_{i}$,
then we contract the tensor network. The resulting object will give us the set of all optimal configurations.
By slightly modifying the above algebra, it can also be used to obtain just a single configuration to save computational effort.
We leave this as an exercise for readers.

\subsection{bounding the enumeration space}
When we try to implement the above algebra for enumerating configurations, we find the space overhead is larger than than we have expected.
It stores more than nessesary intermedite configurations. To speed up the computation, we use $\alpha(G)$ that much easier to compute for bounding.
We first compute the value of $\alpha(G)$ with tropical numbers and cache all intermediate tensors.
Then we compute a boolean masks for each cached tensor, where we use a boolean true to represent a tensor element having contribution to the maximum independent set (i.e. with a nonzero gradient) and boolean false otherwise.
Finally, we perform masked matrix multiplication using the new element type with the above algebra for obtaining all configurations.
To compute the masks, we ``back propagate'' the masks step by step through contraction process using the cached intermediate tensors.
Consider a tropical matrix multiplication $C = A B$, we have the following inequality
\begin{align}
    A_{ij} \odot B_{jk} &\leq C_{ik}.
\end{align}

Moving $B_{ik}$ to the right hand side, we have
\begin{align}
    A_{ij} &\leq (\oplus_{k} (C_{ik}^{-1} \odot B_{jk}))^{-1}
\end{align}
where the tropical multiplicative inverse is defined as the additive inverse of the regular algebra. The equality holds if and only if element $A_{ij}$ contributions to $C$ (i.e. has nonzero gradient).
Let the mask for $C$ being $\overline C$, the backward rule for gradient masks reads
\begin{align}
\overline{A}_{ij} = \delta(A_{ij}, ((C^{\circ-1} \circ \overline C )B^T)_{ij}^{\circ -1}),
\end{align}
where ${}^{\circ -1}$ is the Hadamard inverse, $\circ$ is the Hadamard product, boolean false is treated as tropical zero and boolean true is treated as tropical one.
This rule defined on matrix multiplication can be easily generalized to the einsum of two tensors by replacing the matrix multiplication between $C^{\circ-1} \circ \overline C$ and $B^T$ by an einsum.

\section{Automated branching}
% check http://www.tcs.rwth-aachen.de/independentset/ for more rules
Branching rules can be automatically discovered by contracting the tropical einsum network for a subgraph $R \subseteq G$.
Let us denote the resulting tropical tensor of rank $|C|$ as $A$, where $C$ is the set of boundary vertices defined as $C := \{c | c\in R \land c \in G\backslash R\}$ and $|C|$ the size of $C$.
Each tensor entry $A_{\sigma}$ is a local maximum independant set size with a fixed boundary configuration $\sigma \in \{0,1\}^{|C|}$ by marginalizing the inner degrees of freedom.
If we are only interested in finding a single maximum independent set rather than enumerating all possible solutions,
this tensor can be further ``compresed'' by setting some entries to tropical zero.
Let us define a relation of \textit{less restrictive} as
\begin{align}
(\sigma_a \prec \sigma_b) := (\sigma_a \neq \sigma_b) \land (\sigma_a \leq^\circ \sigma_b)
\end{align}
where $\leq^\circ$ is the Hadamard less or equal operations.
We say an entry $A_{\sigma_a}$ is ``better'' than $A_{\sigma_b}$ if
\begin{align}
(\sigma_a \prec \sigma_b) \land (A_{\sigma_a} \geq A_{\sigma_b}),\label{eq:compress}.
\end{align}
If we remove such $A_{\sigma_b}$, the contraction over the whole graph is guaranted to give the same maximum independant set size.
It can be seen by considering two entries with the same local maximum independent set sizes and different boundary configurations as shown in \Fig{fig:compressrule} (a) and (b).
If we have $\sigma_b \cup \overline{\sigma_b}$ being one of the solutions for maximum independant sets in $G$, then $\sigma_a \cup \overline{\sigma_b}$ is another solution giving the same $\alpha(G)$.
Hence, we can set $A_{\sigma_b}$ to tropical zero safely.
%among boundary configurations with equal local maximum independent sizes,
%we only retain those least restrictive (less ones at the boundary) to exterial configurations.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth, trim={5cm 4cm 5cm 4cm}, clip]{compressionrule.pdf}
    \caption{Two configurations with the same local independent size $A_{\sigma_a} = A_{\sigma_b} = 3$ and different boundary configurations (a) $\sigma_a=\{001\}$ and (b) $\sigma_b = \{101\}$, where black nodes are $1$s (in the independent set) and white nodes are $0$s (not in the independent set).}\label{fig:compressrule}
\end{figure}

\begin{theorem}
    We denote a tensors $A$ as MIS-compact if are no two nonzero entries of it that one is ``better'' than another.
    A MIS-compact tropical tensor is optimal, i.e. none of its none zero entries can be removed without accessing global information.
\end{theorem}

\begin{proof}
    %Given $A_\{\sigma_a}$ is better than $A_\{\sigma_b\}$, any exterior configuration $\overline{\sigma}$ making $\overline{\sigma} \cup \sigma_b$ a global maximum independent set also makes $\overline{\sigma} \cup \sigma_a$ a maximum independent set.
    Let use prove it by showing $\forall \sigma$ in a MIS-compact tropical tensor for a subgraph $R$, there exists a graph $G$ that $R\subseteq G$ and $\sigma$ is the only boundary configuration that produces the maximum independent set.
    i.e. no tensor entry can be removed without knowledge about $G\backslash R$.
    Let $A$ be a tropical tensor, and an entry of it being $A_{\sigma}$, where $\sigma$ is the bounary configuration.
    Let us construct a graph $G$ such that for a vertex $v \in C$, if $\sigma_v=1$, $\alpha(N[v] \cap (G \backslash R)) = 0$, otherwise, $\alpha(N[v] \cap (G\backslash R)) = \infty$, meanwhile, for any $v, w \in C$, $N[v]\cap N[w] = \emptyset$.
    The simplest construction is connecting vertices that $\sigma_v=0$ with infinite many mutually disconnected vertices as illustrated in the following graph.

    \centerline{\includegraphics[width=0.35\textwidth, trim={0cm 2cm 6cm 0cm}, clip]{proofoptimal.pdf}}

    Then we have the maximum independent set size with boundary configuration $\sigma$ being $\alpha(G,\sigma) = \infty (|C|-|\sigma|) + A_{\sigma}$,
    where $|\sigma|$ is defined as the number of $1$s in $\sigma$.
    Let us assume there exists another configuration $\tau$ that generating the same or even better maximum independent set size $\alpha(G, \tau) \geq \alpha(G, \sigma)$.
    Then we have $\tau \prec \sigma$, otherwise it will suffer from infinite punishment from $G\backslash R$.
    For such a $\tau$, we have $A_\tau < A_\sigma$, otherwise $A_\sigma \prec A_\tau$ contradicts with $A$ being MIS-compact.
    Finally, we have $\alpha(G,\tau) = \infty (|C|-|\sigma|) + A_{\tau} < \alpha(G,\sigma)$, which contradicts with our preassumtion. Such $\tau$ does not exist and $\sigma$ is the only boundary configuration that $\alpha(G) = \alpha(G, \sigma)$.
\end{proof}

\subsection{The tensor network compression detects branching rules automatically}
In the following, we are going to show tropical tensor networks with least restrictive principle can automatically discover branching rules.
We denote the effective branching number of contracting the local degrees of freedoms as $\left|\{A_{\sigma} \neq \mymathbb{0}\right|\sigma \in \{0, 1\}^{|C|}\}|/2^{|R|}$.
It is the effective degree of freedoms per vertex in $R$.

\begin{corollary}\label{rule:one} % basic
  If a vertex $v$ is in an independent set $I$, then none of its neighbors can be in $I$.
On the other hand, if $I$ is a maximum (and thus maximal) independent set,
and thus if $v$ is not in $I$ then at least one of its neighbors is in $I$.
\end{corollary}

Contract $N[v]$ and the resulting tensor $A$ has a rank $|N(v)|$. Each tensor entry $A_{\sigma}$ corresponds to a locally maximized independant set size with fixed boundary configuration $\sigma \in \{0, 1\}^{|N(v)|}$.
If the boundary configuration is a bit string of 0s, $\sigma_v$ will takes value $1$ to maximize the local independant set size.

\centerline{\includegraphics[width=0.4\columnwidth,trim={0 3.5cm 0 1cm},clip]{../notebooks/basic.pdf}}

After contracting $N[v]$, $v$ becomes an internal degree of freedom.
Applying tensor compression rule \Eq{eq:compress}, the resulting rank 4 tropical tensor is

\begin{align}
    T_{juwk} = \left(\begin{matrix}
        \left(\begin{matrix}
        ~~~~1 & -\infty \\
        -\infty & ~~~~2
        \end{matrix}\right)_{ju}&
        \left(\begin{matrix}
        -\infty & ~~~~2 \\
        ~~~~2 & ~~~~3
        \end{matrix}\right)_{ju}\\
        \left(\begin{matrix}
        -\infty & ~~~~2 \\
        ~~~~2 & ~~~~3
        \end{matrix}\right)_{ju} &
        \left(\begin{matrix}
        ~~~~2 & ~~~~3 \\
        ~~~~3 & ~~~~4
        \end{matrix}\right)_{ju}
    \end{matrix}\right)_{wk}.
\end{align}

The effective branching value is $11^{1/5} \approx 1.6154$, which is larger than the branching number $\tau(1, 5) \approx 1.3247$.
It does not mean the tropical tensor does not find all the branches, if we contract $N^2[v]$.

\begin{corollary}[mirror rule] % 2.7
For some $v \in V$, a node $u \in N^2(v)$ is called mirror of $v$, if $N(v) \backslash N(u)$ is a clique. We denote the set of of a node $v$ mirrors~\cite{Fomin2013} by $M(v)$.
Let $G = (V, E)$ be a graph and $v$ a vertex of $G$. Then
\begin{equation}
\alpha(G) = \max(1 + \alpha(G \backslash N[v]), \alpha(G \backslash (M(v) \cup \{v\})).
\end{equation}
\end{corollary}

This rule states that if $v$ is not in $M$, there exists an MIS $I$ that $M(v)\notin I$.
otherwise, there must be one of $N(v)$ in the MIS (\textit{local maximum rule}).
If $w$ is in $I$, then none of $N(v) \cap N(w)$ is in $I$, then there must be one of node in the clique $N(v)\backslash N(w)$ in $I$ (\textit{local maximum rule}),
since clique has at most one node in the MIS, by moving the occuppied node to the interior, we obtain a ``better'' solution.
%Hence, the \textit{least restrictive principle} captures the mirror rule.

In the following example, since $u\in N^2(v)$ and $N(v) \backslash N(u)$ is a clique, $u$ is a mirror of $v$.

\centerline{\includegraphics[width=0.4\columnwidth,trim={0 3.5cm 0 1cm},clip]{../notebooks/mirror.pdf}}

After contracting $N[v]\cup u$, $v$ becomes an internal degree of freedom.
Applying tensor compression rule \Eq{eq:compress}, the resulting rank 4 tropical tensor is

\begin{align}
    T_{juwk} = \left(\begin{matrix}
        \left(\begin{matrix}
        ~~~~1 & ~~~~2 \\
        -\infty & -\infty
        \end{matrix}\right)_{ju}&
        \left(\begin{matrix}
        -\infty & -\infty \\
        ~~~~2 & -\infty
        \end{matrix}\right)_{ju}\\
        \left(\begin{matrix}
        -\infty & -\infty \\
        -\infty & -\infty
        \end{matrix}\right)_{ju} &
        \left(\begin{matrix}
        -\infty & -\infty \\
        -\infty & -\infty
        \end{matrix}\right)_{ju}
    \end{matrix}\right)_{wk}.
\end{align}

In this case, the effective branching number is $3^{1/5}\approx 1.2457$,
which is smaller than the branching number $\tau(4, 2) = 1.2721$ by simply applying the mirror rule.

\begin{corollary}[satellite rule] % satellite rule
Let $G$ be agraph $v \in V$. A node $u \in N^2(v)$ is called satellite~\cite{Kneis2009} of $v$, if there is some $u' \in N(v)$ such that $N[u'] \backslash N[v] = \{u\}$.
The set of satellites of a node $v$ is denotedby $S(v)$, and we also use the notation $S[v] := S(v) \cup {v}$. Then 
\begin{equation}
\alpha(G) = \max\{\alpha(G \backslash \{v\}), \alpha(G \backslash N[S[v]]) + |S(v)| + 1\}.
\end{equation}
\end{corollary}

This rule can be capture by contracting $N[v] \cup S(v)$.
In the following example, since $u \in N^2(v)$ and $w \in N(v)$ satisfies $N[w] \backslash N[v] = \{u\}$, $u$ is a satellite of $v$.

\centerline{\includegraphics[width=0.4\columnwidth,trim={0 3.5cm 0 1cm},clip]{../notebooks/satellite.pdf}}

After contracting $N[v] \cup u$, both $v$ and $w$ become internal degrees of freedoms.
Applying tensor compression rule \Eq{eq:compress}, the resulting rank 3 tropical tensor is
\begin{align}
    T_{juk} = \left(\begin{matrix}
        \left(\begin{matrix}
        ~~~~1 & ~~~~2 \\
        ~~~~2 & -\infty
        \end{matrix}\right)_{ju}\\
        \left(\begin{matrix}
        -\infty & -\infty \\
        -\infty & -\infty
        \end{matrix}\right)_{ju}
    \end{matrix}\right)_{k}.
\end{align}

There are 3 nonzero entries. The internal configurations of entry $T(j=1, u=0, k=0) = 2$ is $(v=0, w =1)$,
that of entry $T(j=0, u=1, k=0)=2$ is $(v=1, w=0)$, and that of entry $T(j=0, u=0, k=0)=1$ is $(v=1, w=0)$ or $(v=0, w=1)$.
For entry $T(j=0, u=0, k=0)=1$, we post-select the internal degree of freedom as $(v=0, w=1)$.
Then we can see the satellite rule either ${v, u} \in I$ or $v \notin I$ is satisfied.
In this case, the effective branching number is $3^{1/5}\approx 1.2457$. 

\section{discussion}
We introduced in the main text how to compute the indenpendence polynomial, maximum independent set and optimal configurations.
It is interesting that although these properties are global,
they can be solved by designing different element types that having two operations $\oplus$ and $\odot$ and two special elements $\mymathbb{0}$ and $\mymathbb{1}$.
One thing in common is that they all defines a commutative semiring.
Here, we want the $\oplus$ and $\odot$ operations being commutative because we do not want the contraction result of an einsum network to be sensitive to the contraction order.
We show most of the implementation in Appendix \ref{sec:technical}. It is supprisingly short.
The style that we program is called generic programming,
it is about writing a single copy of code, feeding different types into it, and the program computing the result with a proper performance.
It is language dependent feature. If someone want to implement this algorithm in python,
one has to rewrite the matrix multiplication for different element types in C and then export the interface to python.
In C++, users can use templates for such a purpose.
In our work, we chose Julia because its just in time compiling is very powerful that it can generate fast code dynamically for users.
Elements of fixed size, such as the finite field algebra, tropical number, tropical number with counting/configuration field used in the main text can be inlined in an array.
Furthermore, these inlined arrays can be upload to GPU devices for faster generic matrix multiplication implemented in CUDA.jl.

\begin{table}[h!]\centering
\begin{minipage}{0.9\columnwidth}
\ra{1.3}
    \scalebox{1.0}{
        \begin{tabularx}{\textwidth}{bb}\toprule
            \hline
            \textbf{element type}     & \textbf{purpose} \\
            {regular number}     & {counting all indenepent sets} \\
            {tropical number}     & {finding the maximum independent set size} \\
            {tropical number with counting}     & {finding both the maximum independent set size and its degeneracy} \\
            {tropical number with configuration}     & {finding the maximum independent set size and one of the optimal configurations} \\
            {tropical number with multiple configurations}     & {finding the maximum independent set size and all optimal configurations} \\
            {polynomial}     & {computing the indenpendence polynomials exactly} \\
            {complex number}     & {fitting the indenpendence polynomials with fast fourier transformation} \\
            {finite field algebra}     & {fitting the indenpendence polynomials exactly using number theory} \\
            \bottomrule
        \end{tabularx}
    }
    \caption{Tensor element types used in the main text and their purposes.}\label{tbl:generictypes}
\end{minipage}
\end{table}


\bibliographystyle{iclr2021_conference}
\bibliography{refs}

\appendix

\section{Technical guide}\label{sec:technical}
\begin{description}
	\item[OMEinsum] a package for einsum,
	\item[OMEinsumContractionOrders] a package for finding the optimal contraction order for einsum \\ \href{https://github.com/Happy-Diode/OMEinsumContractionOrders.jl}{https://github.com/Happy-Diode/OMEinsumContractionOrders.jl},
	\item[TropicalGEMM] a package for efficient tropical matrix multiplication (compatible with OMEinsum),
	\item[TropicalNumbers] a package providing tropical number types and tropical algebra, one o the dependency of TropicalGEMM,
	\item[LightGraphs] a package providing graph utilities, like random regular graph generator,
	\item[Polynomials] a package providing polynomial algebra and polynomial fitting,
	\item[Mods and Primes] packages providing finite field algebra and prime number generators.
\end{description}

One can install these packages by opening a julia REPL, type \colorbox{lightgray}{\texttt{]}} to enter the \texttt{pkg>} mode and type, e.g.
\begin{lstlisting}
pkg> add OMEinsum LightGraphs Mods Primes FFTW Polynomials TropicalNumbers
\end{lstlisting}

It may supprise you that the Julia implementation of algorithms introduced in the paper is so short that except the bounding and sparsity related parts,
all are contained in this appendix. After installing required packages, one can open a Julia REPL and copy the following code into it.

\lstinputlisting[breaklines]{../democode/demo.jl}

In the above examples, the configuration enumeration is very slow, one should use the optimal MIS size for bounding as decribed in the main text.
We will not show any example about implementing the backward rule here because it has approximately 100 lines of code.
Please checkout our Github repository \href{https://github.com/Happy-Diode/NoteOnTropicalMIS}{https://github.com/Happy-Diode/NoteOnTropicalMIS}.

\section{When tensor network is worse than einsum network}\label{app:tensorbad}

Given a graph

\centerline{\begin{tikzpicture}
    \def\r{0.2}
    \foreach \x in {1,...,5}
        \foreach \y in {1,...,5}
            \filldraw[fill=black] (\x,\y) circle [radius=\r];
    \foreach \x in {1,...,5}
        \foreach \y in {1,...,4}{
            \draw [black,thick] (\x,\y) -- (\x,\y+1);
            \draw [black,thick] (\y,\x) -- (\y+1,\x);
        }
    \foreach \x in {1,...,4}
        \foreach \y in {1,...,4}{
            \draw [black,thick] (\x,\y) -- (\x+1,\y+1);
            \draw [black,thick] (\y+1,\x) -- (\y,\x+1);
        }
\end{tikzpicture}}

Its tensor network representation is

\centerline{\begin{tikzpicture}
    \def\r{0.1}
    \def\a{0.1}
    \foreach \x in {1,...,5}
        \foreach \y in {1,...,5}
            \filldraw[fill=black] (\x,\y) circle [radius=\r];
    \foreach \x in {1,...,5}
        \foreach \y in {1,...,4}{
            \filldraw[fill=black] (\x-\a,\y+0.5-\a) rectangle (\x+\a, \y+0.5+\a);
            \filldraw[fill=black] (\y+0.5-\a,\x-\a) rectangle (\y+0.5+\a, \x+\a);
            \draw [black,thick] (\x,\y) -- (\x,\y+1);
            \draw [black,thick] (\y,\x) -- (\y+1,\x);
        }
    \foreach \x in {1,...,4}
        \foreach \y in {1,...,4}{
            \filldraw[fill=black] (\x+0.3-\a,\y+0.3-\a) rectangle (\x+0.3+\a, \y+0.3+\a);
            \filldraw[fill=black] (\y+0.3-\a,\x+0.7-\a) rectangle (\y+0.3+\a, \x+0.7+\a);
            \draw [black,thick] (\x,\y) -- (\x+1,\y+1);
            \draw [black,thick] (\y+1,\x) -- (\y,\x+1);
        }
\end{tikzpicture}}

Its einsum network representation is

\centerline{\begin{tikzpicture}
    \def\r{0.1}
    \def\a{0.07}
    \def\L{0.6}
    \def\l{0.1}
    \foreach \x in {1,...,5}
        \foreach \y in {1,...,5}{
        \node[fill, regular polygon, 
            regular polygon sides=3, scale=0.3] at (\x, \y) {};
        }
    \foreach \x in {1,...,5}
        \foreach \y in {1,...,5}{
            \ifnum \y < 5
                \filldraw[fill=black] (\x-\a,\y+0.5-\a) rectangle (\x+\a, \y+0.5+\a);
                \filldraw[fill=black] (\y+0.5-\a,\x-\a) rectangle (\y+0.5+\a, \x+\a);
            \fi
            \filldraw [red, opacity=0.3] plot [smooth cycle] coordinates {(\x-\L,\y+\l) (\x-\l,\y+\l) (\x-\l, \y+\L) (\x+\l,\y+\L) (\x+\l,\y+\l) (\x+\L,\y+\l) (\x+\L,\y-\l) (\x+\l,\y-\l) (\x+\l,\y-\L) (\x-\l,\y-\L) (\x-\l,\y-\l) (\x-\L,\y-\l)};
        }
    \foreach \x in {1,...,4}
        \foreach \y in {1,...,4}{
            \filldraw[fill=black] (\x+0.3-\a,\y+0.3-\a) rectangle (\x+0.3+\a, \y+0.3+\a);
            \filldraw[fill=black] (\y+0.3-\a,\x+0.7-\a) rectangle (\y+0.3+\a, \x+0.7+\a);
        }
\end{tikzpicture}}

\end{document}
