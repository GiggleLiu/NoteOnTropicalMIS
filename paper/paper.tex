\documentclass[review, onefignum, onetabnum]{siamart190516}

\usepackage[linesnumbered, ruled, vlined, algo2e]{algorithm2e}
\input{shared}

\title{Computing solution space properties of combinatorial optimization problems via generic tensor networks}

\externaldocument{suppl}

\begin{document}

\maketitle

\begin{abstract}
We introduce a unified framework to compute the solution space properties of a broad class of combinatorial optimization problems. These properties include finding one of the optimum solutions, counting the number of solutions of a given size, and enumeration and sampling of solutions of a given size. Using the independent set problem as an example, we show how all these solution space properties can be computed in the unified approach of generic tensor networks. We demonstrate the versatility of this computational tool by applying it to several  examples, including computing the entropy constant for hardcore lattice gases, studying the overlap gap properties, and analyzing the performance of quantum and classical algorithms for finding maximum independent sets.
\end{abstract}

% REQUIRED
\begin{keywords}
solution space property, tensor networks, maximum independent set, independence polynomial, generic programming, combinatorial optimization 
\end{keywords}

% REQUIRED
% 14N07  	Secant varieties, tensor rank, varieties of sums of powers
\begin{AMS}
  15A69, 05C31, 14N10
\end{AMS}

\section{Introduction}

An important class of problems in graph theory and combinatorial optimization  can be formulated as satisfiability problems involving constraints specified over a vertex and its neighborhood.
These include, for example, the independent set, the cutting problem, the dominating set, the set packing and  the set covering, the vertex coloring problem, the K-SAT,  the clique problem, and the vertex cover problem~\cite{Moore2011}.
These problems have a wide range of applications in scheduling, logistics,
wireless networks and telecommunication, and computer vision, among others~\cite{Butenko2003, Wu2015}.
Finding an optimum solution for these problems is typically NP-hard~\cite{Hastad1996}.
%, which implies they can not to be solved in a time that scales polynomialy  with the size of the problem.

% The most studied aspect of these problems is to find an optimum solution, 
%The most studied version of the above problems is finding an optimal solution maximum/minimum set satisfying the contraints and the corresponding set size,
% which is typically 
%while many of them belong to the hardness category
% NP-hard~\cite{Hastad1996}, i.e.\ it is unlikely to be solved in a time polynomial with the size of the problem.

% Many properties of these problems are in general intractable at large sizes (exponential run time in problem size in the worst case),
 %but it is nevertheless desirable to have high-performance algorithms because of their ubiquitous applications~\cite{Butenko2003, Wu2015}.

%\purple{Is MIS really the most studied NP-hard optimization problem? I would guess it is heavily studied, but not necessarily the most studied.}
%\blue{Here, we haven't mentioned the independent set, here ``version'' means, e.g. counting, enumeration, devision. I think finding one of the largest/smallest set and its cardinality in the above mentioned problems is the most studied?}

In this Article, we introduce a unified framework to compute a broad class of properties 
associated with the solutions of these problems, beyond just finding an optimum solution.
We call them \textit{solution space properties}. In practice, these can be much harder to compute (corresponding e.g.\ to \#P-complete class~\cite{Moore2011}). However, these properties can be crucial for understanding detailed properties of hard combinatorial optimization problems.
% In this paper, we focus on the less studied aspects of computing \textit{solution space properties}, which can be much harder to compute, %; for example, counting independent sets of a general graph is \#P-complete~\cite{Vadhan2001}, 
% but they can be crucial towards understanding hard combinatorial optimization problems. 
These \textit{solution space properties} can include not only the maximum or minimum set size but also the number of sets at a given size, enumeration of all sets at a given size, and direct sampling of such sets when they are too large to be fit into memory.
% For the weighted version of the above problems, it also includes finding the largest or smallest $k$ sets and their sizes.
%are many interesting and hard computational problems concerning various properties of independent sets.
%Solution space property computation allows us to understand the solution space better.
They can be used to understand the hardness of finding an optimum solution for a given problem instance and the performance of a specific solver.  
%in computing the solution for the given instance.
For example, the number of configurations at different sizes can inform how likely a simulated annealing algorithm will be trapped in local minima at certain sizes~\cite{Xu2018}.
%In the past, people use the counting of configurations at different sizes to tell how likely the simulated annealing will be trapped in certain size in the strong defensive alliance problem~\cite{Xu2018}, 
The pair-wise Hamming distance distribution of configurations at a given size can indicate the presence or absence of the overlap gap property~\cite{Gamarnik2013, Gamarnik2019}, which can be used to bound the performance of local optimization algorithms.
%a no-go theorem about when the solution space geometry is formidably
%hard for any local search algorithm.
In a recent experiment based on a Rydberg atom array quantum computer, 
%the authors use 
the counting and the configuration space connectivity information was used to find maximum independent set (MIS) problem instances that are hard for simulated annealing and to evaluate the corresponding quantum algorithm performance \cite{Ebadi2022}.
% In a recent experiment using a Rydberg atom array quantum simulator to find maximum independent sets (MIS) of a graph~\cite{Ebadi2022}, the authors use the counting information and the configuration space connectivity to find instances hard for simulated annealing. %and design better quantum algorithms 
%to solve these instances faster, 
The need for understanding these important aspects of combinatorial optimization motivates us to find methodologies to compute these solution space properties.
%\purple{I would give examples of why we want to compute solution space properties.
%For instance, in statistical physics, computing thermal states and
%observables via the partition function. In complexity theory, proving limits on local algorithms (OGP).}\blue{now?}

To this end, we show how to obtain all of these seemingly unrelated properties in a unified approach using
\textit{generic tensor networks}. Tensor networks are a computational model widely used in condensed matter physics~\cite{Orus2014}, quantum computing~\cite{Markov2008}, big data~\cite{Cichocki2014} and mathematics~\cite{Oseledets2011}.
They are also known as the sum-product networks in probabilistic modeling~\cite{Bishop2006} or \texttt{einsum} in linear algebra libraries such as NumPy~\cite{Harris2020}.
%\purple{I’d write out $2^{{\rm tw}(G)}$}\blue{I feel it is hard to explain line graph without texts.}
Recent progress in simulating quantum circuits with tensor networks~\cite{Gray2021, Pan2021, Kalachev2021} makes it possible to contract a randomly structured sparse tensor network with up to thousands of tensors in a reasonable time.
%\purple{Is the scaling of these algorithms known?}\blue{They are not exact algorithms, the time can be any, typically a few minutes, will it be helpful to mention this?}
In previous studies, the data types of the tensor elements are typically restricted to standard number types such as real numbers and complex numbers.
Here, we extend to \textit{generic tensor networks} by generalizing the tensor element data types to any type that has the algebraic structure of a commutative semiring.
%These two algorithms are related but are not equivalent.
%Dynamic programming is more general purpose, while a tensor network has a richer algebraic structure, and this structure is crucial for solution space property computation.
%\purple{Not clear how first part and second part of sentence are related}\blue{now? or maybe just removing the comparison with dynamic programming?}
%For sparse graphs, the treewidth is usually much smaller than the number of vertices~\cite{Fomin2006}. Our algorithms, on the other hand, are much more versatile than traditional methods and can be used to compute many other properties than just the MIS size.
%The independent set problem is related to physics applications such as in the hard-core lattice gas model~\cite{Dyre2016, Fernandes2007}
%in statistical mechanics and the Rydberg hamiltonian with neutral atoms~\cite{Pichler2018, Ebadi2022};
%Property computation can, for example, be used to understand phase transitions~\cite{Dyre2016, Fernandes2007},
%to identify harder graphs in an ensemble of graphs~\cite{Ebadi2022} and to analyze the presence of the overlap gap property~\cite{Gamarnik2013, Gamarnik2019} related to the clustering phenomenon of large independent sets.
%However, there is a lack of general and versatile tool to compute these different properties of independent sets.
%To keep the discussion focused, 
In what follows, for clarity of presentation, we focus on the independent set problem in the main text, and show how to compute the solution space properties for other combinatorial optimization problems in \Cref{sec:otherproblems}. The latter include cutting, matching, vertex coloring, satisfiability,  dominating set,  set packing,  set covering, and the clique problem.
%\xpurple{More details on which combinatorial optimization problems this applies to?}


The paper is organized as follows.
We first introduce the basic concepts of tensor networks and generic programming in \Cref{sec:tn} and \Cref{sec:generic}.
Then we show how to reduce the independent set problem to a tensor network contraction problem in \Cref{sec:tnmap}. 
Subsequently, we explain how to engineer the element types to compute various solution space properties in \Cref{sec:counting}, \Cref{sec:enumeration}, and \Cref{sec:weighted}. % to count independent sets at a given size, enumerate/sample independent sets at a given size and find largest set sizes in weighted graphs, respectively.
% by computing a number of independent set properties of certain sparse graphs on central processing units (CPUs) and graphics processing units (GPUs) and show its good performance.
Lastly, we provide three example applications in \Cref{sec:examples} to demonstrate the versatility of our tool.
A benchmark to demonstrate the performance of our algorithms can be found in both the \Cref{sec:benchmark} and the code repository.
%In the first example, we show how our method can help computing the entropy constant for some hardcore lattice gases on 2D square lattices. We compute the 2D generalization of Fibonacci integer sequence up to size $39$, which is larger than the previous record $37$.
%In the second example, we analyze the pairwise Hamming distances from configurations directly sampled from large independent sets of King's graphs at $0.8$ filling and three regular graphs. We show the strongest numeric evidence so far of the absence of overlap gap property in King's graph and the existence of Overlap Gap properties in three regular graphs.
%In the last example, we show how our algorithm helps people to understand the performance of a quantum variational algorithms on Rydberg atom arrays to find maximum independent sets. This is a research frontier that understanding which can help people design better quantum algorithms to beat its classical counterpart.
%Our method can also be used to find ``maximal'' independent set properties; 
%a maximal independent set is an independent set that is not a subset of any other independent set, but its size may not be the maximum. 

\section{Tensor networks}\label{sec:tn}
\begin{definition}[Tensor Network~\cite{Cirac2021, Orus2014}]
    A tensor network is a triple of $\mathcal{N} = (\Tensors, \Inputs, \Output)$ where $\Tensors = (\Tensors_1, \ldots, \Tensors_m)$ is a vector of tensors, $\Inputs$ is a map from tensor to a vector of labels $\Inputs: \Tensors_j \mapsto \mathbf{l}_j$, $(\mathbf{l}_j)_i \in \Sigma, i=1,\ldots, \rm{rank}(\Tensors_j)$.
    $\Output$ is a vector of labels for specifying the output.
\end{definition}
A tensor network is composed of a collection of tensors, a collection of labels associated with tensor dimensions, and an indicator of which tensor is the output.
The operation to evaluate a tensor network is called \textit{contraction}, which is defined as a summation of tensor element products over the labels not appearing in the output tensor.
For example, the matrix multiplication is a special tensor network that can be represented as $C_{ik} = A_{ij}B_{jk}$, where $A, B$ and $C$ are matrices (two-dimensional tensors), $ik,ij$ and $ik$ are labels associated with them, and ``$=$'' is the indicator of $C$ being the output, while its contraction is defined as $C_{ik} = \sum_j A_{ij}B_{jk}$.
%can be viewed as a generalization of binary matrix multiplication to nary tensor contraction.
%Generalized Einstein's notation is often used to represent a tensor network, where we use a label in the subscript to represent a degree of freedom.
%One can enumerate these degrees of freedom and accumulate the product of tensor elements to the output tensor.
% Einstein's notation is often used to represent a tensor network, e.g.\ it represents the matrix multiplication between two matrices $A$ and $B$ as $C_{ik} = A_{ij}B_{jk}$,
% where we use a label in the subscript to represent a degree of freedom.
% One can enumerate these degrees of freedom and accumulate the product of tensor elements to the output tensor.
% In the standard Einstein's notation for tensor networks in physics, each index appears precisely twice, either both in input tensors (which will be summed over) or one in a input tensor and another in the output tensor.
% Hence a tensor network can be represented as a open simple graph,
% where a tensor is mapped to a vertex, a label shared by two input tensors is mapped to a normal edge and a label appears in the output tensor is mapped to an open edge.
% In this work, we adopt a more general form of tensor networks that do not restrict how many times a label can appear in tensors. 
The graphical representation of a tensor network is an open (meaning an edge can connect to external vertices) hypergraph, where an input tensor is mapped to a vertex and a label is mapped to a hyperedge that can connect an arbitrary number of vertices, while the labels appearing in the output tensor are open.
Our notation is a minor generalization of the standard tensor network notation used in physics as we do not restrict the number of times a label can appear in the tensors. 
While this generalized form is equivalent in representation power, it can have smaller contraction complexity as will be illustrated in \Cref{sec:tensorbad}.
% because one can easily translate a generalized tensor network to the standard notation by adding $\delta$ tensors, a high dimensional equivalence of the identity matrix.
%However, introducing $\delta$ tensors may significantly increase the complexity of the tensor contraction. We illustrate this subtle point in \Cref{sec:tensorbad}.

\begin{example}
$C_{ijk} = A_{jkm} B_{mil} V_{jm}$ is a tensor network that can be evaluated as $C_{ijk} = \sum_{ml}A_{jkm} B_{mil} V_{jm}$.
Its hypergraph representation is shown below, where we use different colors to represent different hyperedges.

\vspace{1em}
\centerline{\begin{tikzpicture}[
    dot/.style = {circle, fill, minimum size=#1,
                inner sep=0pt, outer sep=0pt},
    dot/.default = 6pt  % size of the circle diameter 
                    ]  
    \def\dx{0};
    \def\r{0.5cm}
    \def\sr{0.15cm}
    \def\ax{0}
    \def\ay{0}
    \def\bx{1}
    \def\by{1}
    \def\cx{1}
    \def\cy{-1}
    \node[color=white,fill=black,dot=\r] at (\ax+\dx,\ax) (a) {A};
    \node[color=white,fill=black,dot=\r] at (\bx+\dx,\by) (b) {B};
    \node[color=white,fill=black,dot=\r] at (\cx+\dx,\cy) (v) {V};
    \node[color=transparent,draw=transparent,dot=0] at (\ax-1,\by) (o1) {};
    \node[color=transparent,draw=transparent,dot=0] at (\ax-1.5,\ay) (o2) {};
    \node[color=transparent,draw=transparent,dot=0] at (\ax-1,\cy) (o3) {};
    \node at (\ax-0.8,\ay) (k) {k};
    \node at (\bx+0.4,\ay) (m) {m};
    \node at (\ax,\cy) (j) {j};
    \node at (\bx+1,\by) (l) {l};
    \node at (\bx-1,\by) (i) {i};
    \draw[color=blue,thick] (i) -- (b);
    \draw[color=blue,thick] (i) -- (o1);
    \draw[color=cyan,thick] (l) -- (b);
    \draw[color=violet,thick] (k) -- (a);
    \draw[color=violet,thick] (k) -- (o2);
    \draw[color=black,thick] (b) -- (m);
    \draw[color=black,thick] (m) -- (a);
    \draw[color=black,thick] (m) -- (v);
    \draw[color=red,thick] (a) -- (j);
    \draw[color=red,thick] (v) -- (j);
    \draw[color=red,thick] (o3) -- (j);
\end{tikzpicture}}
\end{example}

\section{Generic programming tensor contractions}\label{sec:generic}
%\purple{On a second read of this section, I think that the clarity could be improved. My understanding is this is the flow: We would like to use generic programming, where we write code that works regardless of input type, and is optimized on all input types without sacrificing efficiency. In general, we can write code such that a function runs correctly regardless of the input data type. This is easy to do for dynamically typed languages like Python, which only verify and enforce the rules for the input data type at run time (called type-checking). However, when the input data type is only checked at run time (as opposed to when the code is compiled), type-specific optimization cannot be used. High efficiency is only achieved in statically typed languages, where modern compiling technology allows the function to be individually optimized for each input data type (then give C++/Julia examples).}\blue{how about now?}
In previous works relating tensor networks and combinatorial problems~\cite{Kourtis2019, Biamonte2017}, the element types in the tensor networks are limited to standard number types such as floating-point numbers and integers.
%Owning to the development of modern compiling technology, we no longer need to restrict our imagination to standard number types.
%However, the elements in a tensor do not have to be a regular number type, or even a field (since division is not used in tensor contraction).
%One can use the same tensor contraction program for different purposes with different tensor element types.
% We propose to not restricting our imagination to standard number types, but to use a more general element type with certain algebraic property.
We propose to use more general element types with a certain algebraic property.
With different data types, we can solve different problems within the same unified framework.
This idea of using the same program for different purposes is also called generic programming in computer science:

\begin{definition}[Generic programming ~\cite{Stepanov2014}]
   Generic programming is an approach to programming that focuses on designing algorithms and data structures so that they work in the most general setting without loss of efficiency.
\end{definition}
This definition of generic programming covers two major aspects: ``work in the most general setting'' and ``without loss of efficiency''.
By the most general setting, we mean that a single program should work correctly for the most general input data types. 
%\purple{I think the following paragraph could be clarified. When you read the definition of generic programming, it's not clear what ``in the most general setting'' means. My understanding is it just means that the program will work regardless of the input data type - it would be good to say this explicitly. }
%\blue{This definition is from the book I cited, I also do not like ``in the most general setting'', maybe we just use our version?}
%\purple{I also think the way it is written takes a while to understand what dynamically versus statically typed languages are. The definition, “When these languages “see” a new input type, the compiler can recompile the generic program for the new type.” comes rather late in the paragraph.}
%\xpurple{Also, while I know what you mean, concepts like “inlining immutable elements” and “cache miss rate” would be
%lost on someone only with a physics background.}
%This definition of generic programming contains two major aspects: a single program works in the most general setting and efficiency.
%To understand the first aspect on generality,
For example, suppose we want to write a function that raises an element to a power, $f(x, n) := x^n$.
One can easily write a function for standard number types that computes the power of $x$ in $O \left( \log(n) \right)$ steps using the multiply and square trick.
Generic programming does not require $x$ to be a standard number type;
instead, it treats $x$ as an element with an associative multiplication operation $\odot$ and a multiplicative identity $\mymathbb{1}$.
Then, when the program takes a matrix as an input instead of a standard number type, it computes the matrix power correctly without rewriting the program.
The second aspect is about efficiency. For dynamically typed languages such as Python, the type information is not available for type-specific optimizations at the compilation stage.
Therefore, one can easily write code that works for general input types, but the efficiency is not guaranteed; for example, the speed of computing the matrix multiplication between two NumPy arrays with Python objects as elements is much slower than statically typed (i.e. the type information can be accessed at the compilation stage) languages such as C++ and Julia~\cite{Bezanson2012}.
C++ uses templates for generic programming while Julia takes advantage of just-in-time compilation and multiple dispatches.
When these languages ``see'' a new input type, the compiler recompiles the generic program for the new type to generate an efficient binary.
A myriad of optimizations can be done during the compilation. For example, the compiler can optimize the memory layout of immutable elements with fixed sizes in an array to speed up array indexing.
In Julia, if a type is immutable and contains no references to other values, an array of that type can even be compiled to graphics processing units (GPU) for faster computation~\cite{Besard2018}.

This motivates us to identify the most general tensor element type allowed in a tensor network contraction program.
We find that as long as the tensor elements are members of a commutative semiring, the tensor network contraction will be well defined and the result will be independent of the contraction order.
Compared to a field, a commutative semiring does not need not to have an additive inverse and a multiplicative inverse.
Giving up these nice properties of fields has significant implications for tensor computation: tensor network compression algorithms might not be applicable because matrix factorization is NP-hard for commutative semirings~\cite{Shitov2014} and matrix multiplication faster than $O(n^3)$ does not exist for an algebra without an additive inverse~\cite{Kerr1970}.
Here, we only use the commutative properties of an algebra for optimizing the tensor network contraction order.
To define a commutative semiring with the addition operation $\OPLUS$ (not the logical \texttt{XOR} operation in computer science) and the multiplication operation $\odot$ on a set $S$, the following relations must hold for any arbitrary three elements $a, b, c \in S$.
\begin{align*}
(a \OPLUS b) \OPLUS c = a \OPLUS (b \OPLUS c) & \hspace{5em}\text{$\triangleright$ commutative monoid $\OPLUS$ with identity $\mymathbb{0}$}\\
a \OPLUS \mymathbb{0} = \mymathbb{0} \OPLUS a = a &\\
a \OPLUS b = b \OPLUS a &\\
&\\
(a \odot b) \odot c = a \odot (b \odot c)  &   \hspace{5em}\text{$\triangleright$ commutative monoid $\odot$ with identity $\mymathbb{1}$}\\
a \odot  \mymathbb{1} =  \mymathbb{1} \odot a = a &\\
a \odot b = b \odot a &\\
&\\
a \odot (b\OPLUS c) = a\odot b \OPLUS a\odot c  &  \hspace{5em}\text{$\triangleright$ left and right distributive}\\
(a\OPLUS b) \odot c = a\odot c \OPLUS b\odot c &\\
&\\
a \odot \mymathbb{0} = \mymathbb{0} \odot a = \mymathbb{0}
\end{align*}
%We require the algebra being multiplicative commutative because the contraction order optimizer will change the multiplication order of elements, while we require that the contraction order does not change the contraction results.
In the following sections, we will show how to compute solution space properties of independent sets using the same tensor network contraction algorithm by engineering tensor element algebra.
The Venn diagram in \Cref{fig:venn-diagram} shows the different types of algebra we will introduce in the main text and their relation, and \Cref{tbl:generictypes} summarizes which solution space properties can be computed by which tensor element types.



\begin{figure}[th]
   \centering
\centerline{\begin{tikzpicture}[]
    \scriptsize
    \node[draw,fill=lime!80,fill opacity=1, text opacity=1.0,ellipse,minimum width=2cm, minimum height=1cm,inner sep=0pt] at (0, 2.5) (R) {$\mathbbm{R}$};
    \def\dx{-3};
    \node[draw,fill=teal!50,fill opacity=1, text opacity=1.0,ellipse,minimum width=5cm, minimum height=3cm,inner sep=0pt] at (\dx, 0.15) (PN) {\hspace{-3.7cm}Polynomial};
    \node[draw,fill=brown!75,fill opacity=1, text opacity=1.0,ellipse,minimum width=3.5cm, minimum height=1.0cm,inner sep=0pt] at (\dx, 0.7) (P1) {\hspace{-1.0cm}Largest order};
    \node[draw,fill=brown!40,fill opacity=1, text opacity=1.0,ellipse,minimum width=3.5cm, minimum height=1.0cm,inner sep=0pt] at (\dx, -0.4) (P2) {\hspace{-0.8cm}Largest 2 orders};
    \node[draw,fill=brown,fill opacity=1, text opacity=1.0,ellipse,minimum width=0.8cm, minimum height=0.3cm,inner sep=0pt] at (\dx, 1.0) (T) {};
    \node[draw,fill=brown!50,fill opacity=1, text opacity=1.0,ellipse,minimum width=0.8cm, minimum height=0.3cm,inner sep=0pt] at (\dx, -0.1) (T2) {};
    \node at (\dx, -1.2) {$\ldots$};
    \node[above = 1cm] at (T) (textT) {Tropical};
    \node[below = 2cm, left=4cm] (textT2) {Extended Tropical};
    \draw[black,-latex] (textT) -- (T);
    \draw[black,-latex] (textT2) -- (T2);

    % set and set sampler
    \node[draw,fill=yellow,fill opacity=0.5, text opacity=1.0,ellipse,minimum width=5cm, minimum height=3cm,inner sep=0pt] at (0, 0) (SN) {};
    \node[below of=1] at (SN) {Set};
    \node[draw,fill=red!70,fill opacity=0.5, text opacity=1.0,ellipse,minimum width=4cm, minimum height=1.5cm,inner sep=0pt] at (-0.5, 0) (S1) {\hspace{1.5cm}Bit string};
\end{tikzpicture}}

    \caption{The tensor network element types used in this work and their relations.
    The overlap between two ellipses indicates that a new algebra can be created by combining those two types of algebra. ``Largest order'' and ``Largest 2 orders'' mean truncating the polynomial by only keeping its largest or largest two orders.
    The purposes of these element types can be found in \Cref{tbl:generictypes}.}
     \label{fig:venn-diagram}
\end{figure}

\begin{table}[t!]\centering
\begin{minipage}{\columnwidth}
\ra{1.3}
        \begin{tabularx}{\textwidth}{sb}\toprule
            \hline
   \textbf{Element type}     & \textbf{Solution space property} \\
   {$\mathbbm{R}$}     & {Counting of all independent sets} \\
   {Polynomial} (\Cref{eq:polynomial})     & {Independence polynomial} \\
   {Tropical (\Cref{eq:tropical})}    & {Maximum independent set size} \\
   {Extended tropical of order $k$ (\Cref{eq:ext-tropical})}    & {Largest $k$ independent set sizes} \\
   {Polynomial truncated to $k$-th order (\Cref{eq:countingtropical} and \Cref{eq:max2poly})}     & {$k$ largest independent sizes and their degeneracy} \\
   {Set} (\Cref{eq:set})     & {Enumeration of independent sets} \\
   {Sum-Product expression tree} (\Cref{eq:expr})     & {Sampling of independent sets} \\
   {Polynomial truncated to largest order combined with bit string} (\Cref{eq:singleconfig})     & {Maximum independent set size and one of such configurations} \\
   {Polynomial truncated to $k$-th order combined with set} (\Cref{eq:countingtropicalset})    & {$k$ largest independent set sizes and their enumeration} \\
            \bottomrule
        \end{tabularx}
    \caption{Tensor element types and the independent set properties that can be computed using them.}\label{tbl:generictypes}
\end{minipage}
\end{table}

\section{Tensor network representation of independent sets} \label{sec:tnmap}
This section describes the reduction of the independent set problem to a tensor network contraction problem.
An alternative interpretation, perhaps more accessible to physicists, can be found in \Cref{sec:energymodel}, where we introduce the reduction from the energy model of hardcore lattice gases~\cite{Dyre2016, Fernandes2007}.
Let $G=(V,E)$ be an undirected graph with each vertex $v\in V$ being associated with a weight $w_v$.
An independent set $I \subseteq V$ is a set of vertices that for any vertex pair $u,v \in I$, we have $(u, v) \not\in E$.
To reduce the independent set problem on $G$ to a tensor network contraction, we first map each vertex $v\in V$ to a label $s_v \in \{0, 1\}$ of dimension $2$, where we use $0$ ($1$) to denote a vertex is absent (present) in $I$.
For each label $s_v$, we define a parameterized rank-one tensor indexed by it as
\begin{equation}
    W(x_v^{w_v})_{s_v} = \left(\begin{matrix}
        1 \\
        x_v^{w_v}
    \end{matrix}\right)_{s_v},\label{eq:vertextensor}
\end{equation}
where $x_v$ is a variable associated with $v$.
Tensor elements can be indexed by subscripts, e.g.\ $W(x_v^{w_v})_0=1$ is the first element associated with $s_v=0$ and $W(x_v^{w_v})_1=x_v^{w_v}$ is the second element associated with $s_v=1$.
Similarly, for each edge $(u, v) \in E$, we define a matrix $B$ indexed by $s_u$ and $s_v$ as
\begin{equation}
    \qquad \quad 
       B = \left(\begin{matrix}
        1  & 1\\
        1 & 0
    \end{matrix}\right), \label{eq:edgetensor}
\end{equation}
which encodes the independent set constraint since $B_{11} = 0$, meaning the two vertices cannot be both in the independent set. 

The corresponding tensor network contraction can be defined as
\begin{equation}\label{eq:idp}
    P(G, \{x_1^{w_1}, x_{2}^{w_2}, \ldots,x_{|V|}^{w_{|V|}}\}) = \sum\limits_{s_1, s_2, \ldots, s_{|V|} = 0}^{1} \prod\limits_{v\in V} W(x_v^{w_v})_{s_v} \prod\limits_{(u,v) \in E} B_{s_u s_v},
\end{equation}
where the summation runs over all $2^{|V|}$ vertex configurations $\{s_1, s_{2}, \ldots,s_{|V|}\}$ and accumulates the product of tensor elements to the output $P$. 
% It is easy to see that an edge tensor element $B_{s_us_v}$ encodes the independent set constraint. It zeros out the product whenever both $u$ and $v$ are in the independent set.
A vertex tensor element $W(x_v^{w_v})_{s_v}$ contributes a multiplicative factor $x_v^{w_v}$ whenever $v$ is in the set.

\begin{example}\label{eg:twonode}
Here, we show a minimum example of mapping the independent problem of a 2-vertex complete graph K2 (left) to a tensor network (right).

\centerline{\begin{tikzpicture}[
dot/.style = {circle, fill, minimum size=#1,
            inner sep=0pt, outer sep=0pt},
dot/.default = 6pt  % size of the circle diameter 
                ]  
    \def\dx{0};
    \def\r{0.5cm}
    \def\wr{0.25cm}
    \node[dot=\r, fill=black] at (\dx,0) (a) {\color{white}{a}};
    \node[dot=\r, fill=black] at (2.5+\dx,0) (b) {\color{white}{b}};
    \draw [black,thick] (a) -- (b);

    \def\dx{5};
    \def\r{0.25cm}
    \foreach \x/\y/\e in {1.25/0/ab}
        \node[color=black,fill=black,dot=2*\r] at (\x+\dx,\y) (\e) {\scriptsize \color{white}{B}};
    \foreach \x/\y/\v in {0.3/0/a, 2.2/0/b}
        \node[color=black] at (\x+\dx,\y) (s\v) {$s_\v$};
    \foreach \x/\y/\v in {-0.5/0/a, 3.0/0.0/b}{
        \node[dot=\wr, color=black] at (\x+\dx,\y) (w\v) {};
        \node at (\x+\dx,\y+0.5) () {\scriptsize \color{black}{$W(x_\v^{w_\v})$}};
    }
    \draw [cyan,thick] (wa) -- (sa);
    \draw [cyan,thick] (sa) -- (ab);
    \draw [red,thick] (wb) -- (sb);
    \draw [red,thick] (sb) -- (ab);
\end{tikzpicture}}

In the graphical representation of the tensor network on the right panel,
we use a circle to represent a tensor, a hyperedge in cyan color to represent the degree of freedom $s_a$,
and a hyperedge in red color to represent the degree of freedom $s_b$.
Tensors sharing the same degree of freedom are connected by the same hyperedge.
The contraction of this tensor network has the following form:
\begin{equation}
    \begin{split}
    P({\rm K2},\{x_a^{w_a}, x_b^{w_b}\}) &=
    \left(\begin{matrix} 1 & x_a^{w_a} \end{matrix}\right)
    \left(\begin{matrix} 1 & 1\\ 1  & 0 \end{matrix}\right)
    \left(\begin{matrix} 1 \\ x_b^{w_b} \end{matrix}\right)\\
    &= 1 + x_a^{w_a} + x_b^{w_b}.
    \end{split}
\end{equation}
The resulting polynomial represents 3 different independent sets $\{\}$, $\{a\}$, and $\{b\}$, with weights being $0$, $w_a$, and $w_b$ respectively.
\end{example}
 
For a general graph, it is computationally inefficient to evaluate \Cref{eq:idp} by directly summing up the $2^{|V|}$ products.
A better approach to evaluate a tensor network is: to find a good pair-wise tensor contraction order as a binary tree and then contract two tensors at a time by this order.
Let us denote the line graph of the hypergraph representation of a tensor network as $G'$.
A contraction order of this tensor network corresponds to a tree decomposition of $G'$ and the largest intermediate tensor has a rank equal to the width of this tree decomposition~\cite{Markov2008}.
Therefore, an optimal (in terms of space complexity) contraction order corresponds to the tree decomposition of $G'$ with the smallest width (or the treewidth ${\rm tw}(G')$).
In the independent set problem, the treewidth of $G'$ is the same as the treewidth of the graph this tensor network is mapped from. 
In practice, it is difficult to find an optimal contraction order for large tensor networks because finding the treewidth is a well-known NP-hard problem.
However, it is easy to find a close-to-optimal contraction order within typically a few minutes with a heuristic algorithm~\cite{Kourtis2019, Kalachev2021}.
%The pairwise tensor contraction also makes it possible to utilize basic linear algebra subprograms (BLAS) functions to speed up the computation for certain tensor element types.
For large-scale applications, it is also possible to slice over certain degrees of freedom to reduce the space complexity, i.e.\
loop and accumulate over certain degrees of freedom so that one can have a smaller tensor network inside the loop due to the removal of these degrees of freedom.

\begin{example}\label{eg:tensorcontraction}
In this example, we map a 5-vertex graph (left) to a tensor network (right) and show how the contraction order reduces the time and space complexities.
    
\centerline{\begin{tikzpicture}[
dot/.style = {circle, fill, minimum size=#1,
            inner sep=0pt, outer sep=0pt},
dot/.default = 6pt  % size of the circle diameter 
                ]  
    \def\dx{0};
    \def\r{0.25cm}
    \filldraw[fill=black] (\dx,0) circle [radius=\r];
    \filldraw[fill=black] (\dx,1.5) circle [radius=\r];
    \filldraw[fill=black] (1.5+\dx,0) circle [radius=\r];
    \filldraw[fill=black] (1.5+\dx,1.5) circle [radius=\r];
    \filldraw[fill=black] (2.5+\dx,2.5) circle [radius=\r];
    \draw [black,thick] (\dx,0) -- (\dx,1.5);
    \draw [black,thick] (\dx,0) -- (1.5+\dx,0);
    \draw [black,thick] (\dx,1.5) -- (1.5+\dx,1.5);
    \draw [black,thick] (1.5+\dx,0) -- (1.5+\dx,1.5);
    \draw [black,thick] (1.5+\dx,0) -- (\dx,1.5);
    \draw [black,thick] (2.5+\dx,2.5) -- (1.5+\dx,1.5);
    \node[color=white] at (\dx,0) {a};
    \node[color=white] at (\dx,1.5) {b};
    \node[color=white] at (1.5+\dx,0) {c};
    \node[color=white] at (1.5+\dx,1.5) {d};
    \node[color=white] at (2.5+\dx,2.5) {e};
    \def\dx{5};
    \def\r{0.25cm}
    \foreach \x/\y/\e in {0.75/0/ac, 0/0.75/ab, 1.5/0.75/cd, 0.75/1.5/bd, 0.75/0.75/bc, 2/2/de}
        \node[color=white,fill=black,dot=2*\r] at (\x+\dx,\y) (\e) {\scriptsize B};
    \foreach \x/\y/\v in {0/0/a, 0/1.5/b, 1.5/0/c, 1.5/1.5/d, 2.5/2.5/e}
        \node[color=black] at (\x+\dx,\y) (\v) {$s_\v$};
    \foreach \x/\y/\v in {-0.5/-0.5/a, -0.5/2.0/b, 2.0/-0.5/c, 2.0/1.0/d, 3.0/2.0/e}
        \node[color=white,fill=black,dot=\r] at (\x+\dx,\y) (\v\v) {};
    \foreach \x/\y/\v in {-0.5/-0.5/a, -0.5/2.0/b, 2.0/-0.5/c, 2.0/1.0/d, 3.0/2.0/e}
        \node[color=black] at (\x+\dx+0.6,\y) {\scriptsize $W(x_\v^{w_\v})$};
    \draw [cyan,thick] (a) -- (aa);
    \draw [cyan,thick] (a) -- (ab);
    \draw [cyan,thick] (a) -- (ac);
    \draw [blue,thick] (b) -- (bb);
    \draw [blue,thick] (b) -- (ab);
    \draw [blue,thick] (b) -- (bc);
    \draw [blue,thick] (b) -- (bd);
    \draw [red,thick] (c) -- (cc);
    \draw [red,thick] (c) -- (ac);
    \draw [red,thick] (c) -- (bc);
    \draw [red,thick] (c) -- (cd);
    \draw [green,thick] (d) -- (dd);
    \draw [green,thick] (d) -- (bd);
    \draw [green,thick] (d) -- (de);
    \draw [green,thick] (d) -- (cd);
    \draw [orange,thick] (e) -- (ee);
    \draw [orange,thick] (e) -- (de);
\end{tikzpicture}}

One can represent a possible pair-wise contraction of tensors as a binary tree structure:

\centerline{\begin{tikzpicture}[]
    \def\dx{0};
    \def\dy{0};
    \def\d{1};
    \def\a{1.0};
    \def\b{1};
    \def\ya{-1.0};
    \def\yb{-0.8};
    \node[] at (\dx+5*\a, \dy+5*\b) (R1) {$R[n^2]$};
    \node[] at (\dx+3*\a, \dy+4*\b) (S1) {$S_{bc}[n^3]$};
    \node[] at (\dx+2*\a, \dy+3*\b) (Q1) {$Q_{bd}[n^2]$};
    \node[] at (\dx+8.5*\a, \dy+3*\b) (Q2) {$Q_{bc}[n^2]$};
    \node[] at (\dx+\a, \dy+2*\b) (P1) {$P_{d}[n]$};
    \node[] at (\dx+9*\a, \dy+2*\b) (P2) {$P_{bc}[n^3]$};
    \node[] at (\dx+0.5*\d, \dy+\b) (M1) {$M_{d}[n^2]$};
    \node[] at (\dx+3.5*\d, \dy+\b) (M2) {$M_{bd}[n^2]$};
    \node[] at (\dx+5.5*\d, \dy+\b) (M3) {$M_{cd}[n^2]$};
    \node[] at (\dx+9.5*\d, \dy+\b) (M4) {$M_{ac}[n^2]$};
    \node[] at (\dx, \dy) (Bde) {$B_{de}$};
    \node[] at (\dx+\d, \dy) (We) {$W_e$};
    \node[] at (\dx+2*\d, \dy) (Wd) {$W_d$};
    \node[] at (\dx+3*\d, \dy) (Bbd) {$B_{bd}$};
    \node[] at (\dx+4*\d, \dy) (Wb) {$W_b$};
    \node[] at (\dx+5*\d, \dy) (Bcd) {$B_{cd}$};
    \node[] at (\dx+6*\d, \dy) (Wc) {$W_c$};
    \node[] at (\dx+7*\d, \dy) (Bbc) {$B_{bc}$};
    \node[] at (\dx+8*\d, \dy) (Bab) {$B_{ab}$};
    \node[] at (\dx+9*\d, \dy) (Bac) {$B_{ac}$};
    \node[] at (\dx+10*\d, \dy) (Wa) {$W_a$};
    \draw[] (Bde) -- (M1);
    \draw[] (We) -- (M1);
    \draw[] (Bbd) -- (M2);
    \draw[] (Wb) -- (M2);
    \draw[] (Bcd) -- (M3);
    \draw[] (Wc) -- (M3);
    \draw[] (Bac) -- (M4);
    \draw[] (Wa) -- (M4);
    \draw[] (M1) -- (P1);
    \draw[] (Wd) -- (P1);
    \draw[] (Bab) -- (P2);
    \draw[] (M4) -- (P2);
    \draw[] (P1) -- (Q1);
    \draw[] (M2) -- (Q1);
    \draw[] (Bbc) -- (Q2);
    \draw[] (P2) -- (Q2);
    \draw[] (Bbc) -- (Q2);
    \draw[] (P2) -- (Q2);
    \draw[] (Q1) -- (S1);
    \draw[] (M3) -- (S1);
    \draw[] (S1) -- (R1);
    \draw[] (Q2) -- (R1);
\end{tikzpicture}}
% \small{
% \begin{align*}
%     &\sum_{s_a,s_b,s_c,s_d,s_e} W(x_a, w_a)_{s_a} W(x_b, w_b)_{s_b} W(x_c, w_c)_{s_c} W(x_d, w_d)_{s_d} W(x_e, w_e)_{s_e} B_{s_a s_b} B_{s_b s_d} B_{s_c s_d} B_{s_a s_c} B_{s_b s_c} B_{s_d s_e}\\
%     =&\sum_{s_b,s_c}\left(\sum_{s_d}\left(\left(\left(\left(\sum_{s_e}B_{s_d s_e}W(x_e,w_e)_{s_e}\right) W(x_d,w_d)_{s_d}\right) \left(B_{s_bs_d} W(x_b, w_b)_{s_b}\right)\right) \left(B_{s_cs_d} W(x_c, w_c)_{s_c}\right)\right)\right.\\
%     &\phantom{XXX}\left.\left(B_{s_bs_c}\left(\sum_{s_a}B_{s_as_b}\left(B_{s_as_c}W(x_a,w_a)_{s_a}\right)\right)\right)\right)\\
%     =&1 + x_a^{w_a} + x_b^{w_b} + x_c^{w_c} + x_d^{w_d} + x_e^{w_e} + x_a^{w_a}x_d^{w_d} + x_a^{w_a}x_e^{w_e} + x_c^{w_c}x_e^{w_e} + x_b^{w_b}x_e^{w_e}
%     %=&1+5x+4x^2 \qquad \quad (x_{i} = x, w_{i}=1).
% \end{align*}}
The contraction process goes from bottom to top, where the root node stores the contraction result; the leaves are input tensors and the rest of the nodes are all intermediate contraction results.
Tensor subscripts are indices so that the number of subscripts indicates the space complexity to store this tensor.
The contraction complexity to generate a tensor is annotated in the square brackets, where $n$ is the dimension of the degree of freedoms, which is $2$ in a tensor network mapped from an independent set problem.
One can easily check the largest tensor in contraction has a space complexity $O(n^2)$ and this is the smallest among all possible contraction trees, i.e.\ the treewidth of the original $5$-vertex graph is $2$. The time complexity is $O(n^3)$, which is much smaller than that of direct evaluation $O(n^5)$.
\end{example}

\section{Independence polynomial}\label{sec:indpoly}

\begin{theorem}
    The independence polynomial of a graph $G = (V, E)$ can be computed in time $O(|V|^22^{{\rm tw}(G)})$.
\end{theorem}
Let $x_i = x$ and $w_i = 1$, \Cref{eq:idp} corresponds to the independence polynomial~\cite{Harvey2018,Ferrin2014}:
\begin{equation}\label{eq:idpdef}
I(G, x) = \sum_{k=0}^{\alpha(G)} a_k x^k,
\end{equation}
where $a_k$ is the number of independent sets of size $k$, $\alpha(G) \equiv \max_{I}\sum_{v\in I}w_v$ is the size of a maximum independent set and it is also called the independence number.
An independence polynomial is a useful graph characteristic related to, for example, the partition functions~\cite{Lee1952,Yang1952} and Euler characteristics of the independence complex~\cite{Bousquet2008, Levit2009}.
%Its connection to \Cref{eq:idp} can be understood as follows: the product over vertex tensor elements produces a factor $x^k$, where $k=\sum_i s_i$ counts the set size,
%and the product over edge tensor elements gives a factor $1$ for a configuration being in an independent set and $0$ otherwise. The summation counts the number of independent sets of size $k$. 
By assigning a real number to $x$, one can evaluate this independence polynomial for this specific value directly using tensor network contraction.
For example, the total number of independent sets can be evaluated as $I(G, 1)$.
However, instead of evaluating this polynomial for a certain value, we are more interested in knowing the coefficients of this polynomial, because this quantity tells us the counting of independent sets at different sizes.
%first elevate the tensor elements $0$s and $1$s in tensors $W(x)$ and $B$ from the standard number types to the additive identity, $\mymathbb{0}$, and multiplicative identity, $\mymathbb{1}$, of a commutative semiring as discussed in \Cref{sec:generic}.
%\purple{I'd
%say that you're about to describe this semiring, otherwise it seems like you're going to
%use the general definition of the semiring rather than a specific type of semiring}\blue{from $1$ to $\mymathbb{1}$ and $0$ to $\mymathbb{0}$ applies for a general semiring, I haven't change this part, let's discuss to make sure I understand your point.}\purple{Perhaps start the sentence with: Our semiring elements are a polynomial type...}
To this end, let us create a polynomial number data type by representing a polynomial $a_0 + a_1 x + \ldots + a_k x^k$ as a coefficient vector $a = (a_0, a_1, \ldots, a_k) \in \mathbb{R}^k$, e.g. $x$ is represented as $(0, 1)$.
Then we can define an algebra among coefficient vectors, including a redefinition of additive identity and multiplicative identity.
To avoid potential confusion, let us denote the additive identity as $\mymathbb{0}$, and the multiplicative identity is $\mymathbb{1}$.
The algebra between the polynomials number $a$ of order $k_a$ and $b$ of order $k_b$ is defined as
\begin{equation}
    \eqname{PN}
    \begin{split}
    a \OPLUS b &= (a_0 + b_0, a_1 + b_1, \ldots, a_{\max(k_a, k_b)} + b_{\max(k_a, k_b)}),\\
    a \odot b &= (a_0 + b_0, a_1b_0 + a_0b_1, a_{2}b_{0} + a_{1}b_{1} + a_{0}b_{2},  \ldots, a_{k_a} b_{k_b}),\\
    \mymathbb{0} &= (),  \\
    \mymathbb{1} &= (1), \label{eq:polynomial}
    \end{split}
\end{equation}
where $\OPLUS$ and $\odot$ are the standard polynomial addition and multiplication operations.
The multiplication can be evaluated in time $O(k\log(k))$ using the convolution theorem~\cite{Schonhage1971}, where $k=\max(k_a, k_b)$.
The tensors $W$ and $B$, which are introduced in the previous section (\Cref{sec:tnmap}), can thus be written as 
\begin{equation}
    W^{\rm PN} = \left(\begin{matrix}
        \mymathbb{1} \\
        (0,1)
    \end{matrix}\right),   
    \qquad \qquad
        B^{\rm PN} = \left(\begin{matrix}
        \mymathbb{1}  & \mymathbb{1} \\
        \mymathbb{1} & \mymathbb{0}
    \end{matrix}\right).
\end{equation}
By contracting the tensor network with this polynomial type, we have the exact representation of the independence polynomial.
In practice, using the polynomial type suffers a space overhead proportional to $\alpha(G)$ because each polynomial requires a vector of such size to store the coefficients. 
One may argue that one can first evaluate this polynomial at different $x$ being a real number,
and then apply the Gaussian elimination procedure to fit the coefficients of this polynomial.
This seemingly more time and space-efficient approach suffer from precision issues in practice.
The data ranges of standard integer types are too small to cover many practical use cases,
while the floating-point numbers may have round-off errors that are much larger than the value itself.
These are because the number of independent sets at different sizes may vary by tens or even hundreds of orders of magnitude.
For practical methods to evaluate these coefficients, we refer readers to \Cref{sec:finitefield}, where we provide an accurate and memory-efficient method to find the polynomial by contracting and fitting on finite field algebra.
For simplicity, we use this less efficient polynomial algebra for the discussion in the main text.

% the purpose of discussion in the main text, let us stick to this less efficient polynomial algebra.

\section{Maximum independent sets and its counting}\label{sec:counting}
\subsection{Tropical algebra for finding the MIS size and counting MISs}
\begin{theorem}
    Let $G = (V, E)$ be a graph, its maximum independent set size $\alpha(G)$ can be computed in time $O(|V|2^{{\rm tw}(G)})$.
\end{theorem}
\begin{theorem}
    The number of $k$ independent sets of a graph $G = (V, E)$ can be computed in time $O(k|V|2^{{\rm tw}(G)})$.
\end{theorem}
%The method we use to count the MISs is based on the following observations:
Let $x=\infty$, the independence polynomial in the previous section becomes
\begin{equation}
I(G, \infty) = \lim_{x\rightarrow \infty}\sum_{k=0}^{\alpha(G)}a_k x^k = a_{\alpha(G)} \infty^{\alpha(G)},
\end{equation}
where all terms except the one with the largest order vanish. We can thus replace the polynomial type $a = (a_0, a_1, \ldots, a_k)$ with a new type that has two fields: the largest exponent $k$ and its coefficient $a_k$.
From this, we can define a new algebra as
\begin{equation}
    \eqname{P1}
\begin{split}
    a_x\infty^x \OPLUS a_y\infty^y &= \begin{cases}
        (a_x + a_y)\infty^{\max(x,y)}, & x = y\\
        a_y\infty^{\max(x,y)}, & x < y\\
        a_x\infty^{\max(x,y)}, & x > y
    \end{cases}, \\
    a_x\infty^x \odot a_y\infty^y &= a_x a_y\infty^{x+y}\\
    \mymathbb{0} &= 0\infty^{-\infty}\\
    \mymathbb{1} &= 1\infty^{0}.
\end{split}%& \text{\rlap{(P1)}}
\label{eq:countingtropical}
\end{equation}
Here, we have generalized the previous polynomial to the Laurent polynomial to define the zero-element properly.
To implement this algebra programmatically, we create a data type with two fields $(x, a_x)$ to store the MIS size and its counting,
and define the above operations and constants correspondingly.
If one is only interested in finding the MIS size, one can drop the counting field.
The algebra of the exponents becomes the max-plus tropical algebra~\cite{Maclagan2015, Moore2011}:
\begin{equation}\eqname{T}
    \begin{split}
        x \OPLUS y &= \max(x,y)\\
        x \odot y &= x + y\\
        \mymathbb{0} &= -\infty\\
        \mymathbb{1} &= 0.
    \end{split}\label{eq:tropical}
\end{equation}
Algebra \Cref{eq:tropical} and \Cref{eq:countingtropical} are the same as those used in Liu et al.~\cite{Liu2021} to compute the spin glass ground state energy and its degeneracy.
For independent set calculations here, the vertex tensor and edge tensor becomes:
\begin{equation}
    W^{\rm T}(w_v) = \left(\begin{matrix}
        w_v \\
        \infty
    \end{matrix}\right),   
    \qquad \qquad
        B^{\rm T} = \left(\begin{matrix}
        \mymathbb{1}  & \mymathbb{1} \\
        \mymathbb{1} & \mymathbb{0}
    \end{matrix}\right).
\end{equation}

\subsection{Truncated polynomial algebra for counting independent sets of large size}
Instead of counting just the MISs, one may also be interested in counting the independent sets with the largest several sizes.
For example, if one is interested in counting only $a_{\alpha(G)}$ and $a_{\alpha(G)-1}$, we can define a truncated polynomial algebra by keeping only the largest two coefficients in the polynomial in \Cref{eq:polynomial} as:
\begin{equation}
    \eqname{P2}
    \begin{split}
    a \OPLUS b &= (a_{\max(k_a, k_b)-1} + b_{\max(k_a, k_b)-1}, a_{\max(k_a, k_b)} + b_{\max(k_a, k_b)}),\\
    a \odot b &= (a_{k_a-1} b_{k_b}+a_{k_a} b_{k_b-1}, a_{k_a} b_{k_b}),\\
    \mymathbb{0} &= (), \\
    \mymathbb{1} &= (1).\label{eq:max2poly}
    \end{split}
\end{equation}
In the program, we thus need a data structure that contains three fields, the largest order $k$, and the coefficients for the two largest orders $a_k$ and $a_{k-1}$.
This approach can clearly be extended to calculate more independence polynomial coefficients and is more efficient than calculating the entire independence polynomial.
Similarly, one can also truncate the polynomial and keep only its smallest several orders.
It can be used, for example, to count the maximal independent sets with the smallest cardinality, where a maximal independent set is an independent set that cannot be made larger by adding a new vertex into it without violating the independence constraint.
As will be shown below, this algebra can also be extended to enumerate those large-size independent sets.

\section{Enumerating and sampling independent sets}\label{sec:enumeration}
\begin{theorem}
    Let $G = (V, E)$ be a graph and $M$ be the number of independent sets with size $>\alpha(G)-k$.
    The independent sets with size $>\alpha(G)-k$ of $G$ can be computed in time $O(M|V|2^{{\rm tw}(G)})$.
\end{theorem}

\subsection{Set algebra for configuration enumeration}

The configuration enumeration of independent sets include, for example, the enumeration of all independent sets, the enumeration of all MISs, and the enumeration of independent sets with the largest several sizes.
Recall that in the definition of a vertex tensor in \Cref{eq:vertextensor}, variables carry labels, so that one can read out all independent sets directly from the output polynomials.
The multiplication between labeled variables is commutative while the summation of labeled variables forms a set.
Intuitively, one can use a bit string as the representation of a labeled variable and use the bit-wise or $\lor^\circ$ as the multiplication operation.
For example, in a 5-vertex graph, $x_2$ and $x_5$ can be represented as $01000$ and $00001$ respectively and their multiplication $x_2x_5$ can be represented as $01000 \lor^\circ 00001 = 01001$.
To enumerate all independent sets, we designed an algebra on sets of bit strings:
\begin{equation}
\eqname{SN}
\begin{split}
    s \OPLUS t &= s \cup t\\
    s \odot t &= \{\sigma \lor^\circ \tau \, | \, \sigma \in s, \tau \in t\}\\
    \mymathbb{0} &= \{\}\\
    \mymathbb{1} &= \{0^{\otimes |V|}\},
\end{split}\label{eq:set}
\end{equation}
where $s$ and $t$ are each a set of $|V|$-bit strings.
\begin{example}\label{eg:setalgebra}
    For elements being bit strings of length $5$, we have the following set algebra
\begin{equation*}
\begin{split}
    &\{00001\} \OPLUS \{01110, 01000\} = \{01110, 01000\} \OPLUS \{00001\} = \{00001,01110, 01000\}\\
    &\{00001\} \OPLUS \{\} = \{00001\}\\
&\\
    &\{00001\} \odot \{01110, 01000\} = \{01110, 01000\} \odot \{00001\} = \{01111, 01001\}\\
    &\{00001\} \odot \{\} = \{\}\\
    &\{00001\} \odot \{00000\} = \{00001\}.
\end{split}
\end{equation*}
\end{example}

To enumerate all independent sets,  we initialize variable $x_{i}$ in the vertex tensor to $x_i = \{\boldsymbol{e}_{i}\}$, where $\boldsymbol{e}_i$ is a basis bit string of size $|V|$ that has only one non-zero value at location $i$.
The vertex and edge tensors are thus
\begin{equation}
    W^{\rm SN}(\{\boldsymbol{e}_{i}\}) = \left(\begin{matrix}
        \mymathbb{1} \\
        \{\boldsymbol{e}_{i}\}
    \end{matrix}\right),   
    \qquad \qquad
        B^{\rm SN} = \left(\begin{matrix}
        \mymathbb{1}  & \mymathbb{1} \\
        \mymathbb{1} & \mymathbb{0}
    \end{matrix}\right).
\end{equation}

This set algebra can serve as the coefficients in \Cref{eq:polynomial} to enumerate independent sets of all different sizes, \Cref{eq:countingtropical} to enumerate all MISs, or \Cref{eq:max2poly} to enumerate all independent sets of sizes $\alpha(G)$ and $\alpha(G)-1$.
As long as the coefficients in a truncated polynomial are members of a commutative semiring, the polynomial itself is a commutative semiring.
For example, to enumerate only the MISs, we can define a combined element type $s_{k}\infty^k$, where the coefficients follow the algebra in \Cref{eq:set} and the exponents follow the max-plus tropical algebra.
The combined operations become: 
\begin{equation}
\eqname{P1+SN}
\begin{split}
    s_x\infty^x \OPLUS s_y\infty^y &= \begin{cases}
        (s_x \cup s_y)\infty^{\max(x,y)}, & x = y\\
        s_y\infty^{\max(x,y)}, & x < y\\
        s_x\infty^{\max(x,y)}, & x > y
    \end{cases},\\
    s_x\infty^x \odot s_y\infty^y &= \{\sigma \lor^\circ \tau | \sigma \in s_x, \tau \in s_y\}\infty^{x+y},\\
    \mymathbb{0} &= \{\}\infty^{-\infty},\\
    \mymathbb{1} &= \{0^{\otimes |V|}\}\infty^{0}. \label{eq:countingtropicalset}
\end{split}
\end{equation}
Clearly, the vertex tensor and edge tensor become
\begin{equation}
    W^{\rm P1+SN}(\{\boldsymbol{e}_{i}\}\infty^{w_v}) = \left(\begin{matrix}
        \mymathbb{1} \\
        \{\boldsymbol{e}_{i}\}\infty^{w_v}
    \end{matrix}\right),   
    \qquad \qquad
        B^{\rm P1+SN} = \left(\begin{matrix}
        \mymathbb{1}  & \mymathbb{1} \\
        \mymathbb{1} & \mymathbb{0}
    \end{matrix}\right).
\end{equation}
The enumeration of all MIS configurations corresponds to the contraction of this tensor network.
However, direct contraction might have significant space overheads for keeping too many intermediate states irrelevant to the final maximum independent sets.
We introduce the bounding technique in \Cref{sec:bounding} to avoid this issue.
One may also be interested in the more studied maximal independent sets~\cite{Bron1973, Eppstein2010, Johnson1988} enumeration.
It will be discussed in \Cref{sec:maximal} since it requires using a different tensor network structure.

If one is interested in obtaining only one MIS configuration, they can just keep one configuration in each tensor element to save the computational effort.
By replacing the sets of bit strings in \Cref{eq:set} with a single bit string, we have the following algebra

\begin{equation}
\eqname{S1}
\begin{split}
    \sigma \OPLUS \tau &= \texttt{select}(\sigma, \tau), \\
    \sigma \odot \tau &= (\sigma\lor^\circ \tau),\\
    \mymathbb{0} &= 1^{\otimes |V|}, \\
    \mymathbb{1} &= 0^{\otimes |V|}.
\end{split}\label{eq:singleconfig}
\end{equation}
The \texttt{select} function picks one of $\sigma$ and $\tau$ by some criteria.
It can be picking the one smaller in the lexicographical order such that the addition operation is commutative and associative.
In most cases, it is completely fine for the \texttt{select} function to pick a random one (not commutative and associative anymore) to generate a random MIS.

\subsection{Sampling extremely large configuration space}
\begin{theorem}
    Let $G = (V, E)$ be a graph, its independent sets with size $>\alpha(G)-k$ of can be unbiasedly sampled in time $O(|V|2^{{\rm tw}(G)}) + O(|V|M)$, where $M$ is the number of samples.
\end{theorem}

When the problem scale becomes larger, a set of all bitstrings might be impossible to fit into any type of storage.
To get something meaningful out of the configuration space, we use a binary sum-product expression tree as a compact representation of a set of configurations, i.e.\ instead of directly computing a set using the algebra in \Cref{eq:set}, we store the process of computing it.
Each node in this tree is a quadruple $(type, data, left, right)$, where $type$ is one of \texttt{LEAF}, \texttt{ZERO}, \texttt{SUM} and \texttt{PROD}, $data$ is a bit string as the content in a \texttt{LEAF} node, and $left$ and $right$ are left and right operands of a \texttt{SUM} or \texttt{PROD} node.

\begin{equation}
\eqname{EXPR}
\begin{split}
    s \OPLUS t &= (\texttt{SUM}, /, s, t)\\
    s \odot t &= (\texttt{PROD}, /, s, t)\\
    \mymathbb{0} &= (\texttt{ZERO}, /, /, /)\\
    \mymathbb{1} &= (\texttt{LEAF}, 0^{\otimes |V|}, /, /).
\end{split}\label{eq:expr}
\end{equation}
The vertex tensor and edge tensor become
\begin{equation}
    W^{\rm EXPR}((\texttt{LEAF},\boldsymbol{e}_i,/,/)) = \left(\begin{matrix}
        \mymathbb{1} \\
        (\texttt{LEAF},\boldsymbol{e}_i,/,/)
    \end{matrix}\right),   
    \qquad
        B^{\rm EXPR} = \left(\begin{matrix}
        \mymathbb{1}  & \mymathbb{1} \\
        \mymathbb{1} & \mymathbb{0}
    \end{matrix}\right).
\end{equation}

This algebra is a commutative semiring because we define the equivalence of two sum-product expression trees by comparing their expanded (using \Cref{eq:set}) forms rather than their storage.
Except using the sum-product expression tree directly as tensor elements, one can also let it be the coefficients of a (truncated) polynomial to compute such trees for independent sets with the largest several sizes.
Although likely, one cannot collect all configurations represented by a sum-product expression tree into a vector due to its extremely large size,
they can draw unbiased samples from it by direct sampling.
The sampling program starts from the root node and descends this tree recursively to the left and right siblings.
If a node has type \texttt{SUM}, the program draws samples from the left and right siblings with a probability decided by the size of each sub-tree and returns the union of samples.
Otherwise, if a node has type \texttt{PROD}, the program draws two sets of samples of equal sizes from its left and right siblings and returns the element-wise multiplication ($\lor^\circ$) of them.
The recursion stops at a \texttt{LEAF} node having size $1$ or a \texttt{ZERO} node having size $0$.
In a sum-product expression tree, the number of configurations of sub-trees can be determined easily as we will show in the following example.
\begin{example}
Let us consider the following sum-product expression tree 
\begin{equation*}
    (\texttt{SUM}, /, (\texttt{PROD}, /, A, B), (\texttt{SUM}, /, C, D)),
\end{equation*}
where sub-trees $A, B, C$ and $D$ can be any of the four types of nodes.
This sum-product expression tree can be represented diagrammatically as the following:

\centerline{\begin{tikzpicture}[]
    \def\dx{0};
    \def\a{1.0};
    \def\b{0.5};
    \def\ya{-1.0};
    \def\yb{-0.8};
    \node[] at (\dx, 0) (P1) {$\OPLUS$};
    \node[] at (\dx+\a, \ya) (P2) {$\OPLUS$};
    \node[] at (\dx-\a, \ya) (M1) {$\odot$};
    \node[] at (\dx-\a-\b, \ya+\yb) (A) {$A$};
    \node[] at (\dx-\a+\b, \ya+\yb) (B) {$B$};
    \node[] at (\dx+\a+\b, \ya+\yb) (C) {$C$};
    \node[] at (\dx+\a-\b, \ya+\yb) (D) {$D$};
    \draw[] (P1) -- (M1);
    \draw[] (P1) -- (P2);
    \draw[] (P2) -- (C);
    \draw[] (P2) -- (D);
    \draw[] (M1) -- (A);
    \draw[] (M1) -- (B);
\end{tikzpicture}}

The left and right siblings of the root node have sizes $|A|\times |B|$ and $|C|+|D|$ respectively, while the root node size can be computed as $|A|\times |B| + |C|+|D|$.
The sizes of $A, B, C$, and $D$ can be computed recursively until the program meets either a \texttt{LEAF} node or a \texttt{ZERO} node, which has a known size of $1$ or $0$.
\end{example}

\section{Weighted graphs}\label{sec:weighted}
\begin{theorem}
    Let $G = (V, E, W)$ be a weighted graph, its $k$ largest independent sets can be computed in time $O(k|V|2^{{\rm tw}(G)})$.
\end{theorem}

All the solution space properties and the corresponding algebra on unweighted graphs still hold for integer-weighted graphs, while for general weighted graphs, the independence polynomial is not well defined anymore.
For general weighted graphs, it is more useful to know the $k$ maximum weighted sets and their sizes.
They can be computed by the extended tropical algebra, which is a natural generalization of the max-plus tropical algebra:
\begin{equation}
\eqname{T$k$}
\begin{split}
    s \OPLUS t &= \texttt{largest}(s \cup t, k)\\
    s \odot t &= \texttt{largest}(\{a+b | a \in s, b\in t\}, k)\\
    \mymathbb{0} &= -\infty^{\otimes k}\\
    \mymathbb{1} &= -\infty^{\otimes k-1} \otimes 0
\end{split}\label{eq:ext-tropical}
\end{equation}
where $\texttt{largest}(s, k)$ means truncating the set $s$ by only keeping its $k$ largest values.
The computation of $s \odot t$ is a maximum sum combination problem that can be done in time $O(k\log(k))$.
An efficient algorithm for computing the maximum sum combination problem can be found in \Cref{sec:maxsum}.
The vertex tensor and edge tensor become
\begin{equation}
    W^{{\rm T}k}(-\infty^{\otimes k-1} \otimes w_v) = \left(\begin{matrix}
        \mymathbb{1} \\
        -\infty^{\otimes k-1} \otimes w_v
    \end{matrix}\right),   
    \qquad
        B^{{\rm T}k} = \left(\begin{matrix}
        \mymathbb{1}  & \mymathbb{1} \\
        \mymathbb{1} & \mymathbb{0}
    \end{matrix}\right),
\end{equation}
where we have used the equation $(-\infty^{\otimes k-1} \otimes 1)^{w_v} = -\infty^{\otimes k-1} \otimes w_v$.
To find solutions corresponding to the largest $k$ sizes, one can combine the extended tropical algebra with the bit string algebra (\Cref{eq:singleconfig}).
Since the $\OPLUS$ operation of the configuration sampler will not be used in the combined algebra, the resulting configurations are deterministic and complete.

\section{Example applications} \label{sec:examples}
%In this section, we give a few examples where the different properties of independence sets are used.

\subsection{Number of independent sets and entropy constant for hardcore lattice gases}\label{sec:entropy}
We compute the counting of all independent sets for graphs shown in \Cref{fig:lattices}, where vertices are all placed on square lattices of dimensions $L \times L$.
The types of graphs include: the square lattice graphs (\Cref{fig:lattices}(a)); the square lattice graphs with a filling factor $p=0.8$, which means $\lfloor pL^{2} \rceil$ sites are occupied with vertices  (\Cref{fig:lattices}(b));
the King's graphs  (\Cref{fig:lattices}(c)); the King's graphs with a filling factor $p = 0.8$  (\Cref{fig:lattices}(d)), which is the ensemble of graphs used in Ref.~\cite{Ebadi2022} to benchmark quantum algorithms on a Rydberg atom array quantum computer. 

The number of independent sets for square lattice graphs of size $L \times L$ form a well-known integer sequence (\href{https://oeis.org/A006506}{OEIS A006506}), which is thought as a two-dimensional generalization of the Fibonacci numbers.
We computed the integer sequence for $L=38$ and $L=39$, which is, to the best of our knowledge, not known before.
In the computation, we used finite-field algebra for contracting integer tensor networks with arbitrarily high precision. 

A theoretically interesting number that can be computed using the number of independent sets is the entropy constant, which can describe the thermodynamic properties of hard-core lattice gases at the high-temperature limit.
For the square lattice graphs, this number is called the \textit{hard square entropy constant} (\href{https://oeis.org/A085850}{OEIS A085850}), which is defined as $\lim_{L\rightarrow \infty} F(L,L)^{1/L^2}$, where $F(L,L)$ is the number of independent sets of a given lattice dimensions $L \times L$.
This quantity arises in statistical mechanics of hard-square lattice gases~\cite{Baxter1980, Pearce1988} and is used to understand phase transitions for these systems. This entropy constant is not known to have an exact representation, but it is accurately known in many digits. Similarly, we can define entropy constants for other lattice gases. In \Cref{fig:hardsquare}, we look at how $F(L,L)^{1/\lfloor pL^2\rceil}$ scales as a function of the grid size $L$ for all types of graphs shown in \Cref{fig:lattices}. Our results match the known results for the non-disordered square lattice and King's graphs. For disordered square lattice and King's graphs with a filling factor $p=0.8$, we randomly sample 1000 graph instances. To our knowledge, the entropy constants for these disordered graphs have not been studied before. They may be used to study phase transitions for disordered lattices, which are typically much harder to understand. Interestingly, the variations due to different random instances are negligible for this quantity. 
% \blue{@Shengtao We either keep this example simple by keeping only square and king's graphs, or explain better why we care about entropy constants at filling 0.8.}

\begin{figure}[t] 
    \centering
    \includegraphics[width=\textwidth, trim={0cm 1cm 0cm 1cm}, clip]{figures/lattices.pdf}
    \caption{The types of graphs used in the case study in \Cref{sec:entropy}.
    The lattice dimensions are $L\times L$. (a) square lattice graphs. (b) square lattice graphs with a filling factor $p=0.8$.
    (c) King's graphs. (d) King's graphs with a filling factor $p=0.8$.}
    \label{fig:lattices}
\end{figure}

\begin{figure}[t] 
    \centering
    \includegraphics[width=\textwidth, trim={0cm 0cm 0cm 0cm}, clip]{figures/fig5.pdf}
    \caption{Mean entropy for lattice gases on graphs defined in \Cref{fig:lattices}.
    We sampled $1000$ instances for $p=0.8$ lattices and the error bar is too small to be visible.
    The horizontal black dashed lines are for $\lim_{L\rightarrow \infty} F(L,L)^{1/L^2}$ for the corresponding non-disordered square lattice and King's graphs.
    }
    \label{fig:hardsquare}
\end{figure}

\subsection{The overlap gap property\label{sec:overlap-gap}}
With this tool to enumerate or sample configurations, one can understand the structure of the independent set configuration space, such as the optimization landscape for finding the MISs.
One of the known barriers to finding the MIS is the so-called overlap gap property~\cite{Gamarnik2013, Gamarnik2019}.
If the overlap gap property is present, it means every two large independent sets either have a significant intersection or a very small intersection;
it implies that large independent sets are clustered together.
This clustering property has been used to rigorously prove upper bounds on the performance of local search algorithms~\cite{Gamarnik2013, Gamarnik2019}.
To investigate the overlap gap property, we compute pair-wise Hamming distance distributions of large independent sets as they are good indicators of the presence or absence of overlap gap properties.
We inspect two types of graphs that are particularly interesting, the King's graphs with defects and $3$-regular graphs.
It is known that the MIS problem on a general graph can be mapped to the King's graph with defects~\cite{Garey1977,Ebadi2022}. However, it is not clear whether the MIS problem defined on a randomly generated King's graph with defects can have the overlap gap property.
It is known that finding MISs of a $d$-regular graphs has the overlap gap property~\cite{Rahman2017,Gamarnik2021} when both $d$ and the graph sizes are large, but, it is not known whether, for small $d$, e.g.\ for $3$-regular graphs, this statement remains true.
We randomly generated $9$ instances for each category of King's graph at $0.8$ filling with size $20\times 20$ ($320$ vertices) and $3$-regular graphs with size $110$.
At this problem size, independent sets are too many to fit into any storage, hence we combine the truncated polynomial and sum-product expression tree to directly sample from the target configuration space.
For each instance $G$, we sample $10^4$ pairs of configurations from the independent sets of sizes $\geq \lceil \gamma \times \alpha(G)\rceil$ and show the pair-wise Hamming distance distribution in \Cref{fig:hamming}.
We observe a clear single peak structure at a fixed distance normalized by the MIS size for the King's graphs, indicating the absence of the overlap gap property in a random King's graph at $0.8$ filling.
Since the MIS problem on an arbitrary graph can be mapped to a King's graph at a certain filling, this result is highly nontrivial. It likely implies 
that the King's graphs with defects mapped from hard MIS instances have a very small measure in the total defected King's graph space. In contrast, 
very different pair-wise Hamming distributions are obtained in \Cref{fig:hamming}(b), where we observed the multiple peak structure when the control parameter $\gamma$ is big enough. It indicates the existence of disconnected clusters in the configuration space of the MIS problem on $3$-regular graphs.
We expect this numerical tool can be used to understand this phenomenon better and to further investigate the graph properties and the geometry of the configuration spaces for a variety of graph instances.
\begin{figure} 
    % \begin{subfigure}[t]{\textwidth}
    %     \centering
    %     %\includegraphics[width=\textwidth, trim={0cm 0cm 0cm 0cm}, clip]{figures/fig3.pdf}
    %     \includegraphics[width=0.3\textwidth, trim={0.5cm 0cm 0.5cm 1cm}, clip]{figures/grid-diag-size20d3-k3-alpha0.1-n10000.pdf}
    %     \includegraphics[width=0.3\textwidth, trim={0.5cm 0cm 0.5cm 1cm}, clip]{figures/grid-diag-size20d3-k3-alpha0.05-n10000.pdf}
    %     \includegraphics[width=0.3\textwidth, trim={0.5cm 0cm 0.5cm 1cm}, clip]{figures/grid-diag-size20d3-k3-alpha0.025-n10000.pdf}
    %     \caption{Nine King's graphs of size $20\times 20$, $0.8$ filling.}
    % \end{subfigure}
    % \begin{subfigure}[t]{\textwidth}
    %     \centering
    %     %\includegraphics[width=\textwidth, trim={0cm 0cm 0cm 0cm}, clip]{figures/fig3.pdf}
    %     \includegraphics[width=0.3\textwidth, trim={0.5cm 0cm 0.5cm 1cm}, clip]{figures/grid-regular-size110d3-k3-alpha0.1-n10000.pdf}
    %     \includegraphics[width=0.3\textwidth, trim={0.5cm 0cm 0.5cm 1cm}, clip]{figures/grid-regular-size110d3-k3-alpha0.05-n10000.pdf}
    %     \includegraphics[width=0.3\textwidth, trim={0.5cm 0cm 0.5cm 1cm}, clip]{figures/grid-regular-size110d3-k3-alpha0.025-n10000.pdf}
    %     \caption{Nine random three regular graphs of size $110$.}
    % \end{subfigure}
    \includegraphics[width=\textwidth, trim={0.0cm 1cm 0.0cm 0cm}, clip]{figures/fig6.pdf}
    \caption{Pairwise Hamming distances distribution for configurations sampled from independent sets with sizes $\geq \lceil\gamma \times \alpha(G)\rceil$.
    In each plot, the $x$-axis is the Hamming distance normalized by the total number of vertices and the $y$-axis is the probability.
    }
    \label{fig:hamming}
\end{figure}

%\subsection{Visualizing experimental quantum algorithm outputs for solving Maximum Independent Set}
\subsection{Analyzing quantum and classical algorithms for Maximum Independent Set}
\begin{figure} 
    \centering
    \includegraphics[width=.65\textwidth, trim={0cm 0cm 0cm 0cm}, clip]{figures/exp_configurations.pdf}
    \caption{Visualization of experimental outputs of a quantum algorithm for solving the maximum independent set problem~\cite{Ebadi2022}.
    Each vertex represents an independent set, and each edge represents a pair of independent sets that differ by a swap operation or a vertex addition.}
    \label{fig:exp_configuratoins}
\end{figure}
In a recent work, 
the ability to enumerate configurations and compute independence polynomials was critical in understanding the performance of quantum optimization algorithms for the MIS problem on a Rydberg atom quantum computer~\cite{Ebadi2022}. This work focused on 
%, the authors found that
exploring King's graphs with $0.8$ filling.   The hardest instances for classical simulated annealing could be accurately predicted from the independence polynomial, which gave information about the density of local minima at different independent set sizes.
On the hardest graph instances for simulated annealing studied in the experiment, a high density of local minima were found at independent set sizes of $\alpha(G)-1,$ which the algorithm became trapped in instead of finding the optimal solution of size $\alpha(G)$.
%Initially, it was suspected that simulated annealing trapped certain clusters of these local minima in the solution space. However, 
By enumerating the configurations using techniques described in the present work, we found that %n found that there was no clustering in the solution space as in \Cref{sec:overlap-gap}, and instead 
simulated annealing randomly explores the independent sets of size $\alpha(G)-1$ until an optimum solution is found.
Therefore, the large ratio of local to global minima  prevents simulated annealing from efficiently finding an MIS.
%and studying the solution space properties for independent sets of size $\alpha(G)$ and $\alpha(G)-1$, we gained detailed insights to performance of both the quantum algorithm and simulated annealing, and could determine if these algorithm were likely to return independent sets from certain clusters of local or global minima in the solution space. Instead, we found that simulated annealing becomes trapped in independent sets of size $\alpha(G)-1$ uniformly at random, which is related to observation that there are no disconnected clusters in the solution space for King's graphs in \Cref{sec:overlap-gap}.

Although the performance of the quantum algorithm is more challenging to understand due to the inherent difficulty in studying quantum systems, the present methods allow one 
to gain significant insights by visualizing the experimental outputs of a quantum algorithm over the solution space, as shown in \Cref{fig:exp_configuratoins} for instances with $39$ nodes~\cite{Ebadi2022}.
Here, the structure of the solution space is shown on a graph where each vertex represents a large independent set. Each edge represents a pair of independent sets that differ by a swap operation or a vertex addition to the set, which are the operations naturally present in the effective dynamics at the end of the quantum algorithm. 
%The quantum algorithm either finds the MIS with size $\alpha(G)$ with high probability, or returns a suboptimal independent set, usually with size $\alpha(G)-1.$
The solution space graph is well-connected by local changes to the spin configurations, has a small diameter, and the degree of each node appears to concentrate. 
This visualization makes it clear that 
%the authors inferred that 
the quantum algorithm does not appear to return solely local minima with a large Hamming distance from the MISs as suggested for adiabatic algorithms e.g. by Ref.~\cite{altshuler2010}  which would appear as a long path on the solution space graph from the sampled local minima to the MIS. Instead, the quantum algorithm samples from local minima across the solution space graph with a wide range of Hamming distances from the MISs. In the case where a large superposition state of local minima is created during the coherent evolution, the quantum algorithm achieves a quadratic speedup over simulated annealing.
%Consistent with \Cref{sec:overlap-gap}, there are no disconnected clusters in the solution space for this King's graph.
Looking forward, we expect these tools can be applied to understanding the performance of quantum and classical algorithms on a wide class of NP-hard combinatorial optimization problems.

\section{Discussion and conclusion}
In this work, we introduced a framework that uses generic tensor networks to compute different solution space properties of a certain class of NP-hard combinatorial optimization problems.
%These solution space properties include the best several solutions, the number of solutions of a given size, and the enumeration and sampling of solutions of a given size.
Each solution space property is computed using the same tensor network with different tensor element algebra.
% This technique can further be generalized to computing solution space properties beyond the examples given in this paper. % such as the largest several independent sets sizes, the number of independent sets of a given size, and the enumeration of independent sets of a given size.
%With different algebras, the contraction of the same tensor network gives us the different properties.
The different data types introduced in the main text to compute these properties are summarized in the diagram in \Cref{fig:venn-diagram}.
The class of problems solvable by a tensor network includes but is not limited to maximum independent sets and a variety of other combinatorial problems such as the matching problem, the k-coloring problem, the max-cut problem, the set packing problem, and the set covering problem, as detailed in \Cref{sec:otherproblems}. 

%Other than the case with real numbers, we have polynomials and truncated polynomials.
%We combine them with the bit string algebra, the set algebra, and sum-product expression tree algebra for finding, enumerating, and sampling of independent sets at a given size. Three examples are shown to demonstrate how our method can be used to understand the solution space better.
%In the first example, we show how our method can help computing the entropy constant for some hardcore lattice gases on 2D square lattices. We compute the 2D generalization of Fibonacci integer sequence up to size $39$, which is larger than the previous record $37$.
%In the second example, we analyze the pairwise Hamming distances from configurations directly sampled from large independent sets of King's graphs at $0.8$ filling and three regular graphs. We show the strongest numeric evidence so far of the absence of overlap gap property in King's graph and the existence of Overlap Gap properties in three regular graphs.
%In the last example, we show how our algorithm helps people to understand the performance of a quantum variational algorithms on Rydberg atom arrays to find maximum independent sets. This is a research frontier that understanding which can help people design better quantum algorithms to beat its classical counterpart.

Looking ahead, it could be possible to generalize the idea of generic programming to other algorithms that have certain algebraic structures such as those using the inclusion-exclusion principle or subset convolution~\cite{Fomin2013} and explore what new properties can be computed.
To this end, dynamic programming~\cite{Courcelle1990, Fomin2013} approaches could be considered.
Dynamic programming is closely related to a tropical tensor network~\cite{Liu2021}; for example, the Viterbi algorithm for finding the most probable configuration in a hidden Markov model can be interpreted as a matrix product state featured with tropical algebra, and the tropical tensor network in the main text is potentially equivalent to dynamic programming in finding an optimum solution.
Since dynamic programming has much broader applications, it would be interesting to extend the ideas from this paper to provide an algebraic interpretation for dynamic programming so that it can be used to compute other solution space properties beyond just finding an optimum solution.
It is also possible to extend this idea to other algebras. For example, generic semiring algebra has been used in computational linguishtics to compute interesting quantities of a given grammar and string.~\cite{Goodman1999}.
%Moreover, since the independence polynomial is closely related to the matching polynomial~\cite{Levit2005}, the clique polynomial~\cite{Hoede1994}, and the vertex cover polynomial~\cite{Akbari2013},
%our algorithm to compute the independence polynomial can also be used to compute these graph polynomials.
%By adapting to the style of generic programming, we can implement all the algorithms without much effort.

The source code in Julia language for this paper can be found in the Github repository~\cite{GenericTensorNetworks}. 
There is a short introduction to this repository as well a gist to show how it works in \Cref{sec:technical} as well.
We expect our tool can be used to understand and study many interesting applications of independent sets and beyond.
We also hope the toolkit we built, including tensor network contraction order optimization and efficient tropical matrix multiplication, can be helpful to the development of other scientific software.

%We also hope this approach of generic tensor networks can inspire future works on tensor network computations.

\section*{Acknowledgments}
We would like to thank Pan Zhang for sharing his python code for optimizing contraction orders of a tensor network.
We acknowledge Sepehr Ebadi and Leo Zhou for coming up with many interesting questions about independent sets and their questions strongly motivated the development of this project.
We thank Benjamin Schiffer for providing helpful feedback on the writing of this manuscript.
We thank Chris Elord for helping us write the fastest matrix multiplication library for tropical numbers, TropicalGEMM.jl. 
We thank Jacob Miller for helpful discussions. 
We would also like to thank a number of open-source software developers, including Roger Luo, Time Besard, Edward Scheinerman, and Katharine Hyatt
for actively maintaining their packages and resolving related issues voluntarily.
We acknowledge financial support from the DARPA ONISQ program (grant no.\ W911NF2010021), the Center for Ultracold Atoms, the National Science Foundation, the Vannevar Bush Faculty Fellowship, the U.S. Department of Energy (DE-SC0021013 and DOE Quantum Systems Accelerator Center (contract no.\ 7568717), 
%the Office of Naval Research, 
the Army Research Office MURI. We acknowledge the computation credits provided by Amazon Web Services for running the benchmarks and case studies. Jinguo Liu acknowledges funding support provided by QuEra Computing Inc.\ through a sponsored research program.

\bibliographystyle{siamplain}
\bibliography{refs}

\appendix

\section{An alternative way to construct the tensor network}\label{sec:energymodel}

Let us characterize the independent set problem on graph $G=(V, E)$ as an energy model with two parts
\begin{equation}\label{eq:eng}
    \mathcal{E}(G, s) = -\sum_{i\in V} w_i s_i + \infty \sum_{(i,j) \in E}s_i s_j
\end{equation}
where $s_i$ is a spin on vertex $i \in V$ and $w_i$ is an onsite energy term associated with it.
The first part corresponds to the negative independent set size and the second part describes the independence constraint, which corresponds to the Rydberg blockade~\cite{Pichler2018, Ebadi2022} in cold atom arrays or the repulsive force in hardcore lattice models~\cite{Dyre2016, Fernandes2007}.
The partition function is defined as
\begin{equation}\label{eq:partition}
    \begin{split}
    Z(G, \beta) = \sum_{s}e^{-\beta \mathcal{E}(G, s)} = \sum_{s\in \mathcal{I}(G)} e^{\beta \sum w_i s_i}\\
         = \sum_{k=0}^{\alpha(G)}a(k) e^{\beta k}  \qquad \quad (k = \sum w_i s_i)
    \end{split}
\end{equation}
where $\mathcal{I}(G)$ is the set of independent sets of graph $G$, $\alpha(G)$ is the absolute value of the minimum energy (maximum independent set size), $a(k)$ is the number of spin configurations with energy $-k$ (independent sets of size $k$).
The partition function can be expressed as a tensor network by placing a vertex tensor on each spin $i$
\begin{equation}
    W(e^{\beta w_i}) = \left(\begin{matrix}
        1 \\
        e^{\beta w_i}
    \end{matrix}\right),
\end{equation}
and an edge tensor on each bond
\begin{equation}
       B = \left(\begin{matrix}
        1  & 1\\
        1 & 0
    \end{matrix}\right),
\end{equation}
where the $0$ in the edge tensor comes from $e^{-\beta\infty}$ in the second term of \Cref{eq:eng}, which is the independence constraint.
By letting $x = e^{\beta}$, we get the tensor network for computing the independence polynomial as described by \Cref{eq:vertextensor} and \Cref{eq:edgetensor}.
If we further let $w_i=1$, the second line of \Cref{eq:partition} is equivalent to the independence polynomial.

\section{Hard problems and tensor networks}\label{sec:otherproblems}
\subsection{Maximal independent sets and maximal cliques}\label{sec:maximal}
In this section, we focus the discussion on the maximal independent sets problem since finding maximal cliques of a graph is equivalent to finding the maximal independent sets of the complement of the graph.
Let $G=(V,E)$ be a graph, we denote the neighborhood of a vertex $v\in V$ as $N(v)$.
A maximal independent set $I_m$ is an independent set such that no $v \in V$ satisfies $I_m \cap (\{v\} \cup N[v])  = \emptyset$, i.e.\ an independent set that cannot become a larger one by adding a new vertex.
To characterize the maximal independence restriction, we defined a tensor on each $\{v\}\cup N(v)$ as
\begin{equation}\label{eq:maximal}
    T(x_v^{w_v})_{s_1,s_2,\ldots,s_{|N(v)|},s_v} = \begin{cases}
        s_vx_v^{w_v} & s_1=s_2=\ldots=s_{|N(v)|}=0,\\
        1-s_v& \text{otherwise}.\\
    \end{cases}
\end{equation}
It means if none of the neighbours of $v$ are in $I_{m}$ ($ s_1=s_2=\ldots=s_{|N(v)|}=0$), then $v$ must be in $I_{m}$ and contribute a factor $x_{v}$, otherwise, if any of the neighbourhood vertices is in $I_{m}$, then $v$ cannot be in $I_{m}$.
For a vertex $v$ that has a degree 2, the tensor has the following form
\begin{equation}
    T(x_v^{w_v})=\left(\begin{matrix}
    \left(\begin{matrix}
        ~~0 &~1 \\
        ~~1 &~1
    \end{matrix}\right)\\
    \left(\begin{matrix}
        x_v^{w_v} &0 \\
        0 &0
    \end{matrix}\right)
    \end{matrix}\right).
\end{equation}
 
Let us consider the graph in \Cref{eg:tensorcontraction}. The corresponding tensor network structure for computing the maximal independent polynomial has the following hypergraph representation. \\

    \centerline{\begin{tikzpicture}[
    dot/.style = {circle, fill, minimum size=#1,
                inner sep=0pt, outer sep=0pt},
    dot/.default = 6pt  % size of the circle diameter 
                    ]  
        \def\dx{0};
        \def\r{0.4cm}
        \def\G{1.0}
        \foreach \x/\y/\v in {0/0/a, 1/0/b, 2/0/c, 3/0/d, 4/0/e}
            \node[color=black] at (\x*\G+\dx,\y) (\v) {$s_\v$};
        \foreach \x/\v/\t in {0/A/$T_a$, 1/B/$T_b$, 2/C/$T_c$, 3/D/$T_d$, 4/E/$T_e$}
            \node[color=white,fill=black,dot=\r] at (\x*\G+\dx,1.5) (\v) {\scriptsize \t};
        \draw [cyan,thick] (a) -- (A);
        \draw [cyan,thick] (a) -- (B);
        \draw [cyan,thick] (a) -- (C);
        \draw [blue,thick] (b) -- (B);
        \draw [blue,thick] (b) -- (A);
        \draw [blue,thick] (b) -- (C);
        \draw [blue,thick] (b) -- (D);
        \draw [red,thick] (c) -- (C);
        \draw [red,thick] (c) -- (A);
        \draw [red,thick] (c) -- (B);
        \draw [red,thick] (c) -- (D);
        \draw [green,thick] (d) -- (D);
        \draw [green,thick] (d) -- (B);
        \draw [green,thick] (d) -- (C);
        \draw [green,thick] (d) -- (E);
        \draw [orange,thick] (e) -- (E);
        \draw [orange,thick] (e) -- (D);
    \end{tikzpicture}}
 
%The contraction complexity of this tensor network is often greater than the one for the independence set problem.
By contracting this tensor network with generic element types,
we can compute the maximal independent set properties such as the maximal independence polynomial and the enumeration of maximal independent sets.
The maximal independence polynomial is defined as
\begin{equation}
D_{m}(G, x) = \sum_{k=0}^{\alpha(G)} b_k x^k,
\end{equation}
where $b_k$ is the number of maximal independent sets of size $k$.
Comparing with the independence polynomial in \Cref{eq:idpdef}, we have $b_{k} \leq a_{k}$ and $b_{\alpha(G)} = a_{\alpha(G)}$. $D_m(G, 1)$ counts the total number of maximal independent sets~\cite{Gaspers2012, Manne2013};
to our knowledge, the best algorithm has a time complexity $O(1.3642^{|V|})$~\cite{Gaspers2012}.
%If we want to find an MIS, $b_{k}$ can, in some cases, provide hints on the difficulty of finding the MIS using local algorithms~\cite{Ebadi2022}.
%The uni-modality, log-concavity, and real-rootness properties of the maximal independence polynomial for special classes of graphs have also been studied~\cite{Hu2017}. 

The benchmark of computing the maximal independent set properties on $3$-regular graphs is shown in \Cref{sec:benchmark}.

\subsection{Matching problem}
A $k$-matching in a graph $G=(V,E)$ is a set of $k$ edges that no two of which have a vertex in common.
We map an edge $(u, v) \in E$ to a degree of freedom $\langle u, v\rangle \in \{0, 1\}$ in a tensor network, where $1$ means an edge is in the set and $0$ otherwise.
To characterize the matching constraint, we define a tensor for each $N(v)=\{n_1, n_2,\ldots, n_{d(v)}\}$ as
\begin{equation}
    W_{\langle v, n_1\rangle, \langle v, n_2 \rangle, \ldots, \langle v, n_{d(v)}\rangle} = \begin{cases}
        1, & \sum_{i=1}^{d(v)} \langle v, n_i \rangle \leq 1,\\
        0, & \text{otherwise},
    \end{cases}
\end{equation}
and a tensor for each bond as
\begin{equation}
    B(x^{w_{\langle u,v \rangle}}_{\langle u,v\rangle})_{\langle u, v\rangle} = \begin{cases}
    1, & \langle u, v \rangle = 0 \\
    x^{w_{\langle u,v \rangle}}_{\langle u, v\rangle}, & \langle u, v \rangle = 1,
\end{cases}
\end{equation}
where a label $\langle v, u \rangle$ is equivalent to $\langle u,v\rangle$.
$W$ tensor specifies the constraint that a vertex cannot be shared by two edges in the edge set,
and an edge tensor carries the weights.
Let $x_{\langle u,v\rangle}^{w_{\langle u,v\rangle}}=x$, the tensor network contraction corresponds to the matching polynomial
\begin{equation}
    M(G, x) = \sum\limits_{k=1}^{|V|/2} c_k x^k,
\end{equation}
where $k$ is the size of an edge set, and a coefficient $c_k$ is the number of $k$-matchings.

\subsection{Vertex coloring}
Let $G=(V,E)$ be a graph. A vertex coloring is an assignment of colors to each vertex $v\in V$ such that no edge connects two identically colored vertices. 
In a $k$-coloring problem, the number of colors is limited to less or equal to $k$.
Let us use the 3-coloring problem as an example to show how to reduce it to tensor contractions.
We first map a vertex $v \in V$ to a degree of freedom $c_v\in\{0,1,2\}$.
Then we define a tensor labeled by $c_v$ for each vertex $v$ as
\begin{equation}
    W = \left(\begin{matrix}
        r_v\\
        g_v\\
        b_v
    \end{matrix}\right),
\end{equation}
and a bond tensor labelled by $(c_u, c_v) $ for each edge $(u, v) \in E$ as
\begin{equation}
    B(x^{w_{uv}}) = \left(\begin{matrix}
        0 & x^{w_{uv}} & x^{w_{uv}}\\
        x^{w_{uv}} & 0 & x^{w_{uv}}\\
        x^{w_{uv}} & x^{w_{uv}} & 0
    \end{matrix}\right),
\end{equation}
where $r_v$, $g_v$ and $b_v$ are colors for labeling the configurations. $B$ tensors are for specifying the coloring constraints and $W$ tensors are for labeling the solutions.
%The number of possible colorings can be obtained by contracting this tensor network by setting $r_v, g_v$ and $b_v$ to $1$.
Let $x^{w_{uv}} = x$ and $r_v = g_v = b_v = 1$; we then have a graph polynomial,
in which the $k$-th coefficient is the number of coloring that $k$ bonds satisfy constraints.
If a graph is colorable, the maximum order of this polynomial should be equal to the number of edges in this graph.
Similarly, one can define an edge coloring problem by defining the tensor network on the line graph of $G$.

\subsection{Cutting problem}
In graph theory, a cut is a partition of the vertices of a graph into two disjoint subsets,
which is also known as the spin-glass problem in statistical physics.
Let $G=(V,E)$ be a graph. We associate a weight $w_v$ to each $v\in V$. To reduce the cutting problem on $G$ to the contraction of a tensor network, we first define a Boolean degree of freedom $s_v\in\{0, 1\}$ for each vertex $v\in V$.
Then for each edge $(u,v)\in E$, we define an edge matrix labeled by $(s_u, s_v)$ as
\begin{equation}
    B(x^{w_{uv}}_u, x^{w_{uv}}_v) = \left(\begin{matrix}
        1 & x_{v}^{w_{uv}}\\
        x_{u}^{w_{uv}} & 1
    \end{matrix}\right),
\end{equation}
where variables $x_u^{w_{uv}}$ and $x_v^{w_{uv}}$ are for a cut on this edge or a domain wall in a spin glass problem.
Let $x_u^{w_{uv}} = x_v^{w_{uv}} = x$; we have a graph polynomial similar to the previous ones,
in which the $k$th coefficient is two times the number of cut configurations that have size $k$ (i.e.\ cutting $k$ edges).

\subsection{Dominating Set}
In graph theory, a dominating set for a graph $G = (V, E)$ is a subset $D \subseteq V$ such that every vertex not in $D$ is adjacent to at least one member of $D$.
To reduce this problem to the contraction of a tensor network, we first map a vertex $v\in V$ to a Boolean degree of freedom $s_v\in\{0, 1\}$.
Then for each vertex $v$, we define a tensor on its closed neighborhood $\{v\} \cup N(v)$ as
\begin{equation}
T(x^{w_v}_v)_{s_1,s_2,\ldots,s_{|N(v)|},s_v} = \begin{cases}
    0 & s_1=s_2=\ldots=s_{|N(v)|}=s_v=0,\\
    1 & s_v=0,\\
    x^{w_v}_v & \text{otherwise},
\end{cases}
\end{equation}
where $w_v$ is the weight associated with the vertex $v$.
This tensor implies a configuration having a closed neighborhood of $v$ not in $D$ ($s_1=s_2=\ldots=s_{|N(v)|}=s_v=0$) cannot be a dominating set.
Otherwise, if $v$ is in $D$, this tensor contributes a multiplicative factor $x_v^{w_v}$ to the output.
The graph polynomial for the dominating set problem is known as the domination polynomial~\cite{Alikhani2009}
\begin{equation}
D(G, x) = \sum_{k=0}^{\gamma(G)} d_k x^k,
\end{equation}
where $d_k$ is the number of dominating sets of size $k$.

\subsection{Boolean satisfiability Problem}
The Boolean satisfiability problem is the problem of determining if there exists an assignment that satisfies a given Boolean formula.
One can specify a satisfiable problem in the conjunctive normal form (CNF), i.e.\ a conjunction of clauses (or disjunctions of Boolean literals).
To reduce the problem of solving a CNF to a tensor network contraction, we first map a Boolean literal $a$ and its negation $\neg a$ to the same degree of freedom $s_a \in \{0, 1\}$.
$s_a = 0$ stands for variable $a$ having value \texttt{false} while $s_a=1$ stands for having value \texttt{true}.
Then we map a clause to a tensor. For example, a $k$-th clause $\neg a \vee b \vee \neg c$ can be mapped to a tensor labeled by $(s_a, s_b, s_c)$.
\begin{equation}
C_{k} = \left(\begin{matrix}
\left(\begin{matrix}
x^{w_k} & x_{b}^{w_k} \\
x_a^{w_k} & x_{ab}^{w_k}
\end{matrix}\right) \\
\left(\begin{matrix}
x_{c}^{w_k} & x_{bc}^{w_k} \\
1 & x_{abc}^{w_k}
\end{matrix}\right)
\end{matrix}\right),
\end{equation}
where $w_k$ is a weight associated with a clause. There is only one entry $(s_a, s_b, s_c) = (1, 0, 1)$ that makes this clause unsatisfied.
Let $x^{w_k}_{\ldots} = x$, one can get a polynomial, in which the $k$-th coefficient gives the number of assignments that $k$ clauses are satisfied.

\subsection{Set packing}
Suppose one has a finite set $S$ and a list of subsets of $S$. Then, the set packing problem asks if some $k$ subsets in the list are pairwise disjoint.
It is the hypergraph generalization of the independent set problem, where a set corresponds to a vertex and an element corresponds to a hyperedge.
The generic tensor network for the set packing problem has a similar form as that for the independent set problem, where the vertex tensor is same as \Cref{eq:vertextensor}, while the edge tensor generalizes \Cref{eq:edgetensor} to higher dimensions
\begin{equation}
    B_{s_u,s_v,\ldots, s_w} = \begin{cases}
        1, & s_u+s_v+\ldots+s_w\leq 1,\\
        0, & \text{otherwise}.
    \end{cases}
\end{equation}

\subsection{Set covering}
Given a collection of elements, the set covering problem aims to find the minimum number of sets that incorporate (cover) all of these elements.
In the set covering problem, two sets are given: a set $U$ of elements and a set $S$ of subsets of the set $U$. The union of all the subsets covers the set $U$. For each set $s \in S$, we associate it with an possitive integer weight $w_s$.
To get the generic tensor network representation, we first map a set $s\in S$ to a Boolean degree of freedom $l_s\in\{0, 1\}$.
For each set $s$, we define a parameterized rank-one tensor indexed by $l_s$ as
\begin{equation}
W(x_s^{w_s}) = \left(\begin{matrix}
    1 \\
    x_s^{w_s}
    \end{matrix}\right)
\end{equation}
where $x_s$ is a generic typed variable associated with $s$ and the positive integer power of which can be define by the repeated multiplication.
For each unique element $a \in U$, we can define a constraint over all $s \in S$ containing this element, i.e. $N(a) = \{s | s \in S \land a\in s\}$, as
\begin{equation}
B_{l_1,l_2,\ldots,l_{|N(a)|}} = \begin{cases}
    0 & l_1=l_2=\ldots=l_{|N(a)|}=0,\\
    1 & \text{otherwise}.
\end{cases}
\end{equation}
If a subset of $S$ does not include any sets containing element $a$, then this configuration has zero contribution to the contraction result.

\section{Bounding the MIS enumeration space}\label{sec:bounding}
\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth, trim={0cm 0cm 0cm 0cm}, clip]{figures/masktree.pdf}
    \caption{Bounded enumeration of maximum independent sets. Here, a circle is a tensor, an arrow specifies the execution direction of a function, $\overline A$ is the Boolean mask for $A$ and $\circ$ is the Hadamard (element-wise) multiplication. (a) is the forward pass with tropical algebra (\Cref{eq:tropical}) for computing $\alpha(G)$.
     (b) is the backward pass for computing Boolean gradient masks.
     (c) is the masked tensor network contraction with tropical algebra combined with sets (\Cref{eq:countingtropicalset}) for enumerating configurations.}
     \label{fig:bounding}
\end{figure}

When using the algebra in \Cref{eq:countingtropicalset} to enumerate all MISs, the program often stores significantly more intermediate configurations than necessary.
To reduce the space overhead, we will show how to bound the searching space using the MIS size $\alpha(G)$.
The bounded contraction consists of three stages as shown in \Cref{fig:bounding}. (a) We first compute the value of $\alpha(G)$ with tropical algebra and cache all intermediate tensors.
(b) Then, we compute a Boolean mask for each cached tensor, where we use a Boolean \texttt{true} to represent a tensor element having a contribution to the MIS and Boolean \texttt{false} otherwise.
(c) Finally, we perform masked tensor network contraction (i.e.\ discarding elements masked \texttt{false}) using the element type with the algebra in \Cref{eq:countingtropicalset} to obtain all MIS configurations.
The crucial part is computing the masks in step (b). Note that these masks correspond to tensor elements with non-zero gradients to the MIS size; we can compute these masks by back-propagating the gradients.
To derive the back-propagation rule for tropical tensor contraction,
we first reduce the problem to finding the back-propagation rule of a tropical matrix multiplication $C = A B$.
Since $ C_{ik} = \bigOPLUS_{j} \ A_{ij} \odot B_{jk} = \max_{j} \ A_{ij} \odot B_{jk}$ with tropical algebra, we have the following inequality
\begin{equation}
    A_{ij} \odot B_{jk} \leq C_{ik}.
\end{equation}
Here $\leq$ on tropical numbers are the same as the real-number algebra.
The equality holds for some $j'$, which means $A_{ij'}$ and $B_{j'k}$ have contributions to $C_{ik}$.
Intuitively, one can use this relation to identify elements with nonzero gradients in $A$ and $B$,
but if doing this directly, one loses the advantage of using BLAS libraries~\cite{TropicalGEMM} for high performance.
Since $A_{ij} \odot B_{jk} = A_{ij} + B_{jk}$, one can move $B_{jk}$ to the right hand side of the inequality: 
\begin{equation}
    A_{ij} \leq C_{ik} \odot B_{jk}^{\circ -1}
\end{equation}
where ${}^{\circ -1}$ is the element-wise multiplicative inverse on tropical algebra (which is the additive inverse on real numbers).
The inequality still holds if we take the minimum over $k$: 
\begin{equation}
    A_{ij} \leq \min_{k}(C_{ik} \odot B_{jk}^{\circ -1}) = \left(\max_{k} \left(C_{ik}^{\circ -1} \odot B_{jk} \right) \right)^{\circ -1} = \left(\bigOPLUS_{k} \left(C_{ik}^{\circ -1} \odot B_{jk} \right) \right)^{\circ -1} = \left( C^{\circ-1} B^{\mathsf{T}} \right)^{\circ -1}_{ij}.
\end{equation}
On the right-hand side, we transform the operation into a tropical matrix multiplication so that we can utilize the fast tropical BLAS routines~\cite{TropicalGEMM}.
Again, the equality holds if and only if the element $A_{ij}$ has a contribution to $C$ (i.e.\ having a non-zero gradient).
Let the gradient mask for $C$ be $\overline C$; the back-propagation rule for gradient masks reads
\begin{equation}\label{eq:adrule}
\overline{A}_{ij} = \delta \left(A_{ij}, \left( \left( C^{\circ-1} \circ \overline C \right) B^{\mathsf{T}} \right)_{ij}^{\circ -1} \right),
\end{equation}
where $\delta$ is the Dirac delta function that returns one if two arguments have the same value and zero otherwise, $\circ$ is the element-wise product, Boolean false is treated as the tropical number $\mymathbb{0}$, and Boolean true is treated as the tropical number $\mymathbb{1}$.
This rule defined on matrix multiplication can be easily generalized to tensor contraction by replacing the matrix multiplication between $C^{\circ-1} \circ \overline C$ and $B^{\mathsf{T}}$ by a tensor contraction.
With the above method, one can significantly reduce the space needed to store the intermediate configurations by setting the tensor elements masked false to zero during contraction.

\section{The fitting approach to computing the independence polynomial}\label{sec:finitefield}
In this section, we propose to find the independence polynomial by fitting $\alpha(G)+1$ random pairs of $x_{i}$ and $y_{i} = I(G,x_{i})$. One can then compute the independence polynomial coefficients $a_{i}$ by solving the linear equation: 
\begin{equation}
\left(\begin{matrix}
1 & x_0 & x_0^2 & \ldots & x_0^{\alpha(G)} \\
1 & x_1 & x_1^2 & \ldots & x_1^{\alpha(G)} \\
\vdots & \vdots & \vdots &\ddots & \vdots \\
1 & x_{\alpha(G)} & x_{\alpha(G)}^2 & \ldots & x_{\alpha(G)}^{\alpha(G)}
\end{matrix}\right)
\left(\begin{matrix}
a_0 \\ a_1 \\ \vdots \\ a_{\alpha(G)}
\end{matrix}\right)
= \left(\begin{matrix}
y_0 \\ y_1 \\ \vdots \\ y_{\alpha(G)}
\end{matrix}\right).\label{eq:lineareq}
\end{equation}
Unlike using the polynomial numbers in \Cref{eq:polynomial},  the fitting approach does not have the linear overhead in space.
However, since the independence polynomial coefficients can have a huge order-of-magnitude range, the round-off errors can be larger than the value itself when using floating-point numbers in computation.
To avoid using the arbitrary precision number that can be very slow and is incompatible with GPU devices, we introduce the following finite-field algebra $\text{GF}(p)$ approach:
\begin{equation}
\eqname{GF$(p)$}
\begin{split}
    x ~\OPLUS~ y &= x+y\pmod p,\\
    x ~\odot~ y &= xy\pmod p,\\
    \mymathbb{0} &= 0,\\
    \mymathbb{1} &= 1.
\end{split}\label{eq:finitefield}
\end{equation}
Regarding the finite-field algebra, we have the following observations:
\begin{enumerate}
    \item One can use Gaussian elimination~\cite{Golub2013} to solve the linear equation \Cref{eq:lineareq} since it is a generic algorithm that works for any elements with field algebra. The multiplicative inverse of a finite-field algebra can be computed with the extended Euclidean algorithm.
    \item Given the remainders of a larger unknown integer $x$ over a set of co-prime integers $\{p_1, p_2, \ldots, p_n\}$,
    $x \pmod {p_1 \times p_2 \times \ldots \times p_n}$ can be computed using the Chinese remainder theorem. With this, one can infer big integers from small integers.
\end{enumerate}
With these observations, we develop \Cref{alg:finitefield} to compute the independence polynomial exactly without introducing space overheads.
The algorithm iterates over a sequence of large prime numbers until convergence.
In each iteration, we choose a large prime number $p$, and contract the tensor networks to evaluate the polynomial for each variable $\chi = (x_{0}, x_{1}, \ldots, x_{\alpha(G)})$ on ${\rm GF}(p)$ and denote the outputs as $(y_0, y_1, \ldots, y_{\alpha(G)}) \pmod p$.
Then we solve \Cref{eq:lineareq} using Gaussian elimination on ${\rm GF}(p)$ to find the coefficient modulo $p$, $A_p \equiv (a_0, a_1, \ldots, a_{\alpha(G)})\pmod p$.
As the last step of each iteration, we apply the Chinese remainder theorem to update $A \pmod P $ to $ A \pmod {P\times p}$, where $P$ is a product of all prime numbers chosen in previous iterations.
If this number does not change compared with the previous iteration, it indicates the convergence of the result and the program terminates.
All computations are done with integers of fixed-width $W$ except the last step of applying the Chinese remainder theorem, where we use arbitrary precision integers to represent the counting.

\LinesNumberedHidden
\begin{algorithm}[!ht]
    \small
    \SetAlgoNoLine
    %\LinesNumbered
    Let $P = 1$, $W$ be the integer width, vector $\chi = (0,1,2, \ldots, \alpha(G))$, matrix $X_{ij} = (\chi_i)^j$, where $i,j = 0, 1, \ldots, \alpha(G)$\;

    \While{true}{
        compute the largest prime $p$ that $\gcd(p, P) = 1$ and $p < 2^W$\;

        \For{$i=0\ldots\alpha(G)$}{
            $y_i \pmod p$ = ${\rm contract\_tensor\_network}(\chi_i\pmod p)$ \tcp*[l]{on $\text{GF}(p)$}
        }

        $A_p = (a_0, a_1, \ldots, a_{\alpha(G)}) \pmod p = {\rm gaussian\_elimination}(X, (y_0, y_1, \ldots, y_{\alpha(G)}) \pmod p) $\;

        $A_{P\times p} = {\rm chinese\_remainder}(A_P, A_p)$\;

        \If{$A_P = A_{P \times p}$}{
            \Return $A_P$ \tcp*[l]{converged}
        }
        $P = P \times p$\;
    }\caption{Computing the independence polynomial exactly without integer overflow}\label{alg:finitefield} 
\end{algorithm}

Alternatively, one can use a faster but less accurate Fourier transformation based method to fit this polynomial, which is detailed and benchmarked in \Cref{sec:benchmark}.

\section{Integer sequence formed by the number of independent sets}

We computed the number of independent sets on square lattices and King's graphs with our generic tensor network contraction algorithm on GPUs.
The tensor element type is the finite-field algebra so that we can reach an arbitrary precision.
We also computed the independence polynomial for these lattices up to size $30\times 30$ in our \href{https://github.com/GiggleLiu/NoteOnTropicalMIS/tree/master/data}{Github repository}.


\begin{table}[h]
\caption{The number of independent sets for square lattice graphs of size $L\times L$. This forms the integer sequence \href{https://oeis.org/A006506}{OEIS A006506}.
Here we only include two updated entries for $L=38,39$, which, to our knowledge, has not been computed before~\cite{Butera2014}.
}
\begin{center}
\scalebox{0.9}{
\begin{tabular}{|c| >{\centering\arraybackslash} p{0.95\linewidth}|}
 \hline $L$  & square lattice graphs \\
 \hline $38$ & 616 412 251 028 728 207 385 738 562 656 236 093 713 609 747 387 533 907 560 081 990 229 746 115 948 572 583 817 557 035 128 726 922 565 913 748 716 778 414 190 432 479 964 245 067 083 441 583 742 870 993 696 157 129 887 194 203 643 048 435 362 875 885 498 554 979 326 352 127 528 330 481 118 313 702 375 541 902 300 956 879 563 063 343 972 979\\
 \hline $39$ &  29 855 612 447 544 274 159 031 389 813 027 239 335 497 014 990 491 494 036 487 199 167 155 042 005 286 230 480 609 472 592 158 583 920 411 213 748 368 073 011 775 053 878 033 685 239 323 444 700 725 664 632 236 525 923 258 394 737 964 155 747 730 125 966 370 906 864 022 395 459 136 352 378 231 301 643 917 282 836 792 261 715 266 731 741 625 623 207 330 411 607\\
  \hline
\end{tabular}
}
\end{center}
\label{tbl:squaregrid}
\end{table}
% online digit seperator: https://www.browserling.com/tools/thousands-separator

\end{document}
